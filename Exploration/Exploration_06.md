# Exploration 6 ì˜í™”ë¦¬ë·° í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„í•˜ê¸°

# í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ì˜ ìœ ìš©ì„±

ì˜¤ëŠ˜ì€ IMDbë‚˜ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ì— ë‹´ê¸´ ì´ìš©ìì˜ ê°ì„±ì´ ê¸ì •ì ì¸ì§€ í˜¹ì€ ë¶€ì •ì ì¸ì§€ë¥¼ ë¶„ë¥˜(Classification)í•  ìˆ˜ ìˆëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤.

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled.png)

### Q&A

<aside>
ğŸ’¡ **Q.** **í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ê·¸ ìœ ìš©ì„±ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì–´ë–¤ íŠ¹ì§•ìœ¼ë¡œë¶€í„° ë¹„ë¡¯ë˜ëŠ” ê²ƒì¸ê°€ìš”?**

A. SNS ë“±ì—ì„œ ê´‘ë²”ìœ„í•œ ë¶„ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì´ ë°ì´í„°ëŠ” ì†Œë¹„ìë“¤ì˜ ê°œì¸ì , ê°ì„±ì  ë°˜ì‘ì´ ì§ì ‘ ë‹´ê²¨ ìˆì„ë¿ë”ëŸ¬ ì‹¤ì‹œê°„ íŠ¸ë Œë“œë¥¼ ë¹ ë¥´ê²Œ ë°˜ì˜í•˜ëŠ” ë°ì´í„°ì´ê¸°ë„ í•˜ë‹¤.

</aside>

<aside>
ğŸ’¡ **Q. í…ìŠ¤íŠ¸ ê°ì„±ë¶„ì„ ì ‘ê·¼ë²•ì„ í¬ê²Œ 2ê°€ì§€ë¡œ ë‚˜ëˆ„ë©´ ë¬´ì—‡ê³¼ ë¬´ì—‡ì´ ìˆë‚˜ìš”?

A. ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ë²•ê³¼ ê°ì„±ì‚¬ì „ ê¸°ë°˜ ì ‘ê·¼ë²•**

</aside>

<aside>
ğŸ’¡ **Q. ì‚¬ì „ ê¸°ë°˜ì˜ ê°ì„±ë¶„ì„ì´ ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ë²• ëŒ€ë¹„ ê°€ì§€ëŠ” í•œê³„ì ì„ 2ê°€ì§€ë§Œ ë“¤ì–´ ì£¼ì„¸ìš”.

A. 
1.ë¶„ì„ ëŒ€ìƒì— ë”°ë¼ ë‹¨ì–´ì˜ ê°ì„± ì ìˆ˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê°€ëŠ¥ì„±ì— ëŒ€ì‘í•˜ê¸° ì–´ë µë‹¤.
2.ë‹¨ìˆœ ê¸ë¶€ì •ì„ ë„˜ì–´ì„œ ê¸ë¶€ì •ì˜ ì›ì¸ì´ ë˜ëŠ” ëŒ€ìƒ ì†ì„± ê¸°ë°˜ì˜ ê°ì„± ë¶„ì„ì´ ì–´ë µë‹¤.**

</aside>

<aside>
ğŸ’¡ **Q. ê°ì„±ë¶„ì„ ë“± í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì´ ë‹¤ë¥¸ ë°ì´í„°ë¶„ì„ ì—…ë¬´ì— ì–´ë–¤ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‚˜ìš”?

A. ì¼ë°˜ì ì¸ ë°ì´í„°ë¶„ì„ ì—…ë¬´ëŠ” ë²”ì£¼í™”ê°€ ì˜ ëœ ì •í˜•ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ë°, ì´ëŸ° ë°ì´í„°ë¥¼ í° ê·œëª¨ë¡œ êµ¬ì¶•í•˜ê¸° ìœ„í•´ì„œ ë§ì€ ë¹„ìš©ì´ ë“¤ì§€ë§Œ, ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆëŠ” ë¹„ì •í˜•ë°ì´í„°ì¸ í…ìŠ¤íŠ¸ì— ê°ì„±ë¶„ì„ ê¸°ë²•ì„ ì ìš©í•˜ë©´ í…ìŠ¤íŠ¸ë¥¼ ì •í˜•ë°ì´í„°ë¡œ ê°€ê³µí•˜ì—¬ ìœ ìš©í•œ ì˜ì‚¬ê²°ì • ë³´ì¡°ìë£Œë¡œ í™œìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤.**

</aside>

<aside>
ğŸ’¡ **Q. ë¼ë²¨ë§ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ì˜ ë¹„ìš©ì„ ì ˆê°í•˜ë©´ì„œ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ìì—°ì–´ì²˜ë¦¬ ê¸°ë²•ì—ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?

A. ë‹¨ì–´ì˜ íŠ¹ì„±ì„ ì €ì°¨ì› ë²¡í„°ê°’ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì›Œë“œ ì„ë² ë”©(word embedding) ê¸°ë²•**

</aside>

# í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•

í…ìŠ¤íŠ¸ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ê·¸ ìì²´ë¡œëŠ” ê¸°í˜¸ì¼ ë¿ì´ë©°, í…ìŠ¤íŠ¸ê°€ ë‚´í¬í•˜ëŠ” ì˜ë¯¸ë¥¼ ê¸°í˜¸ê°€ ì§ì ‘ ë‚´í¬í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.

ë”¥ëŸ¬ë‹ì„ í†µí•´ **ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°**ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

> i feel hungry
i eat lunch
now i feel happy
> 

```python
# ì²˜ë¦¬í•´ì•¼ í•  ë¬¸ì¥ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì— ì˜®ê²¨ ë‹´ì•˜ìŠµë‹ˆë‹¤.
sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']

# íŒŒì´ì¬ split() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ìª¼ê°œ ë´…ë‹ˆë‹¤.
word_list = 'i feel hungry'.split()

index_to_word={}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ì„œ

# ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì”© ì±„ì›Œ ë´…ë‹ˆë‹¤. ì±„ìš°ëŠ” ìˆœì„œëŠ” ì¼ë‹¨ ì„ì˜ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‚¬ì‹¤ ìˆœì„œëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
# <BOS>, <PAD>, <UNK>ëŠ” ê´€ë¡€ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ë§¨ ì•ì— ë„£ì–´ì¤ë‹ˆë‹¤. 
index_to_word[0]='<PAD>'  # íŒ¨ë”©ìš© ë‹¨ì–´
index_to_word[1]='<BOS>'  # ë¬¸ì¥ì˜ ì‹œì‘ì§€ì 
index_to_word[2]='<UNK>'  # ì‚¬ì „ì— ì—†ëŠ”(Unknown) ë‹¨ì–´
index_to_word[3]='i'
index_to_word[4]='feel'
index_to_word[5]='hungry'
index_to_word[6]='eat'
index_to_word[7]='lunch'
index_to_word[8]='now'
index_to_word[9]='happy'

print(index_to_word)
'''
{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}
'''

# {í…ìŠ¤íŠ¸:ì¸ë±ìŠ¤} êµ¬ì¡°ë¡œ ë³€ê²½ (ê¸°ì¡´-> {ì¸ë±ìŠ¤:í…ìŠ¤íŠ¸})
word_to_index={word:index for index, word in index_to_word.items()}
print(word_to_index)
'''
{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}
'''
```

ì´ì œ ì´ ë‹¨ì–´ ì‚¬ì „ì— ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ ìˆ«ìë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤.

```python
print(word_to_index['feel'])  # ë‹¨ì–´ 'feel'ì€ ìˆ«ì ì¸ë±ìŠ¤ 4ë¡œ ë°”ë€ë‹ˆë‹¤
# 4
```

í•¨ìˆ˜í™”

- í•œ ë¬¸ì¥ì„ ìˆ«ìë¡œ encodeí•´ì¤€ëŠ” í•¨ìˆ˜
- ì—¬ëŸ¬ ë¬¸ì¥ì„ ìˆ«ìë¡œ encodeí•´ì¤€ëŠ” í•¨ìˆ˜
- í•œ ìˆ«ì ë²¡í„°ë¥¼ ë¬¸ìë¡œ decode í•´ì£¼ëŠ” í•¨ìˆ˜
- ì—¬ëŸ¬ ìˆ«ì ë²¡í„°ë¥¼ ë¬¸ìë¡œ decode í•´ì£¼ëŠ” í•¨ìˆ˜

```python
# ë¬¸ì¥ 1ê°œë¥¼ í™œìš©í•  ë”•ì…”ë„ˆë¦¬ì™€ í•¨ê»˜ ì£¼ë©´, ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.
# ë‹¨, ëª¨ë“  ë¬¸ì¥ì€ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ í•©ë‹ˆë‹¤. 
def get_encoded_sentence(sentence, word_to_index):
    return [word_to_index['<BOS>']]+[word_to_index[word] \
if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]

print(get_encoded_sentence('i eat lunch', word_to_index))
'''
[1, 3, 6, 7]
'''

# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œêº¼ë²ˆì— ìˆ«ì í…ì„œë¡œ encodeí•´ ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. 
def get_encoded_sentences(sentences, word_to_index):
    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]

# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] ê°€ ì•„ë˜ì™€ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤. 
encoded_sentences = get_encoded_sentences(sentences, word_to_index)
print(encoded_sentences)
'''
[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]
'''

# ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. 
def get_decoded_sentence(encoded_sentence, index_to_word):
    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]ë¥¼ í†µí•´ <BOS>ë¥¼ ì œì™¸

print(get_decoded_sentence([1, 3, 4, 5], index_to_word))
'''
i feel hungry
'''

# ì—¬ëŸ¬ ê°œì˜ ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. 
def get_decoded_sentences(encoded_sentences, index_to_word):
    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]

# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ê°€ ì•„ë˜ì™€ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤.
print(get_decoded_sentences(encoded_sentences, index_to_word))
'''
['i feel hungry', 'i eat lunch', 'now i feel happy']
'''
```

# Embedding ë ˆì´ì–´ì˜ ë“±ì¥

í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆê²Œ ë˜ì—ˆì§€ë§Œ ì´ ë²¡í„°ëŠ” í…ìŠ¤íŠ¸ì— ë‹´ê¸´ ì–¸ì–´ì˜ ì˜ë¯¸ë¡¸ ëŒ€ì‘ë˜ëŠ” ë²¡í„°ê°€ ì•„ë‹ˆë¼ ì„ì˜ë¡œ ë¶€ì—¬ëœ ë‹¨ì–´ì˜ ìˆœì„œì— ë¶ˆê³¼í•˜ë‹¤. Tensorflow, Pytorch ë“±ì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë“¤ì€ ì˜ë¯¸ ë²¡í„° íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í˜„í•œ Embedding ë ˆì´ì–´ë¥¼ ì œê³µí•œë‹¤.

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%201.png)

â€˜greatâ€™ëŠ” ë¨¼ì € 1918ì´ë¼ëŠ” indexë¡œ ë³€í™˜ë˜ê³  lookup tableì—ì„œ [1.2, 0.7, 1.9, 1.5]ì™€ ê°™ì€ Embedding vectorë¡œ ë³€í™˜ëœë‹¤.

```python
# ì•„ë˜ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ì‹œë©´ ì—ëŸ¬ê°€ ë°œìƒí•  ê²ƒì…ë‹ˆë‹¤. 

import numpy as np
import tensorflow as tf
import os

vocab_size = len(word_to_index)  # ìœ„ ì˜ˆì‹œì—ì„œ ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ê°œìˆ˜ëŠ” 10
word_vector_dim = 4    # ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ 4ì°¨ì›ì˜ ì›Œë“œ ë²¡í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. 

embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)

# ìˆ«ìë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„° [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] ì— Embedding ë ˆì´ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤. 
raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')
output = embedding(raw_inputs)
print(output)
```

ìœ„ì˜ ì½”ë“œëŠ” ì—ëŸ¬ë¥¼ ë°œìƒí•˜ëŠ” ë° ê·¸ ì´ìœ ëŠ” Embedding ë ˆì´ì–´ì˜ inputì´ ë˜ëŠ” ë¬¸ì¥ ë²¡í„°ëŠ” ê·¸ ê¸¸ì´ê°€ ì¼ì •í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (raw_inputì˜ 3ê°œ ë²¡í„°ì˜ ê¸¸ì´ëŠ” ê°ê° 4,4,5)

Tensorflowì—ì„œëŠ” **`tf.keras.preprocessing.sequence.pad_sequences`** ë¼ëŠ” í¸ë¦¬í•œ í•¨ìˆ˜ë¥¼ í†µí•´ ë¬¸ì¥ ë²¡í„° ë’¤ì— íŒ¨ë”©(<PAD>)ì„ ì¶”ê°€í•˜ì—¬ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì£¼ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

```python
raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,
                                                       value=word_to_index['<PAD>'],
                                                       padding='post',
                                                       maxlen=5)
print(raw_inputs)
'''
[[1 3 4 5 0]
 [1 3 6 7 0]
 [1 8 3 4 9]]
'''
```

ìœ„ì˜ ì½”ë“œ ë‹¤ì‹œì‹œë„

```python
vocab_size = len(word_to_index)  # ìœ„ ì˜ˆì‹œì—ì„œ ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ê°œìˆ˜ëŠ” 10
word_vector_dim = 4    # ê·¸ë¦¼ê³¼ ê°™ì´ 4ì°¨ì›ì˜ ì›Œë“œ ë²¡í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.

embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)

# tf.keras.preprocessing.sequence.pad_sequencesë¥¼ í†µí•´ word vectorë¥¼ ëª¨ë‘ ì¼ì • ê¸¸ì´ë¡œ ë§ì¶°ì£¼ì–´ì•¼ 
# embedding ë ˆì´ì–´ì˜ inputì´ ë  ìˆ˜ ìˆìŒì— ì£¼ì˜í•´ ì£¼ì„¸ìš”. 
raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)
raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,
                                                       value=word_to_index['<PAD>'],
                                                       padding='post',
                                                       maxlen=5)
output = embedding(raw_inputs)
print(output)
'''
tf.Tensor(
[[[ 0.02096364  0.04570582  0.01623174 -0.03731489]
  [-0.00049068  0.03613648 -0.0483637  -0.01599956]
  [ 0.00528395  0.04702011  0.0020753  -0.01195306]
  [ 0.01520946 -0.02078022  0.04988812  0.02934435]
  [ 0.00972203  0.0051379   0.02984723 -0.01997231]]

 [[ 0.02096364  0.04570582  0.01623174 -0.03731489]
  [-0.00049068  0.03613648 -0.0483637  -0.01599956]
  [-0.04748726  0.04907146 -0.03113985  0.00511583]
  [ 0.01047821 -0.04604801  0.04600367 -0.04344996]
  [ 0.00972203  0.0051379   0.02984723 -0.01997231]]

 [[ 0.02096364  0.04570582  0.01623174 -0.03731489]
  [ 0.00446204  0.0051946   0.0275388   0.03405363]
  [-0.00049068  0.03613648 -0.0483637  -0.01599956]
  [ 0.00528395  0.04702011  0.0020753  -0.01195306]
  [ 0.04341961  0.00676854  0.01015236 -0.01791128]]], shape=(3, 5, 4), dtype=float32)
'''
```

<aside>
ğŸ’¡ **Q. outputì˜ shape=(3, 5, 4)ì—ì„œ 3, 5, 4ì˜ ì˜ë¯¸ëŠ” ê°ê° ë¬´ì—‡ì¼ê¹Œìš”?

A. 3ì€ ì…ë ¥ë¬¸ì¥ ê°œìˆ˜, 5ëŠ” ì…ë ¥ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´, 4ëŠ” ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜**

</aside>

# ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” RNN

ì£¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë° ì‚¬ìš©ë˜ëŠ” **Recurrent Neural Network(RNN)** ì€ **ì‹œí€€ìŠ¤(Sequence)** í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ìµœì ì¸ ëª¨ë¸ì´ë‹¤.

â€˜i feel hungryâ€™ë¼ëŠ” ìŒì„±ì„ ì´ˆë‹¹ í•œ ë‹¨ì–´ì”©, 3ì´ˆì— ê±¸ì³ ìŒì„± ë°ì´í„°ë¡œ ì´ì•¼ê¸°í–ˆì„ ë•Œì˜ ì˜ˆë¥¼ ë³´ì.

> at time=0s : ë“£ëŠ”ì´ì˜ ê·€ì— ë“¤ì–´ì˜¨ input='i'
at time=1s : ë“£ëŠ”ì´ì˜ ê·€ì— ë“¤ì–´ì˜¨ input='feel'
at time=2s : ë“£ëŠ”ì´ì˜ ê·€ì— ë“¤ì–´ì˜¨ input='hungry'
> 

ë‹¤ìŒê³¼ ê°™ì´ ì…ë ¥ì´ ì‹œê°„ì˜ ì¶•ì„ ë”°ë¼ ë°œìƒí•˜ëŠ” ë°ì´í„°ê°€ **ì‹œí€€ìŠ¤ ë°ì´í„°**ì´ë‹¤.

ë§Œì•½ times=1sì¸ ì‹œì ì—ì„œ ì…ë ¥ìœ¼ë¡œ ë°›ì€ ë¬¸ìì€ â€˜i feelâ€™ê¹Œì§€ì´ë‹¤. ê·¸ ë‹¤ìŒì— ì–´ë– í•œ ë§ì´ ì˜¬ì§€ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ìƒí™©ì´ë‹¤. RNNì€ ì´ëŸ° ìƒí™©ì„ ë¬˜ì‚¬í•˜ê¸°ì— ê°€ì¥ ì ë‹¹í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ê·¸ ì´ìœ ëŠ” RNNì€ ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ìƒˆë¡­ê²Œ ë“¤ì–´ì˜¤ëŠ” ì…ë ¥ì— ë”°ë¼ ë³€í•˜ëŠ” í˜„ì¬ ìƒíƒœë¥¼ ë¬˜ì‚¬í•˜ëŠ” state machineìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤.

state ì˜ˆì‹œ

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%202.png)

ë‹¤ìŒì˜ ë‘ ëŒ€í™”ë¥¼ ë³´ë©´ Stateful ëŒ€í™”ì—ì„œ ì§ì›ì€ ì†ë‹˜ì´ ì´ì „ì— ì£¼ë¬¸í•œ ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  StatelessëŒ€í™”ì—ì„œëŠ” ê¸°ì–µì„ í•˜ì§€ ëª»í•˜ê³  ìˆë‹¤. 

### ê¹€ì„±í›ˆ êµìˆ˜ì˜ ëª¨ë‘ì˜ ë”¥ëŸ¬ë‹ ê°•ì¢Œ 12ê°•. RNN

- í˜„ì¬ì˜ stateê°€ ê·¸ ë‹¤ìŒ stateì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
- new state = f(old state, input)
- Vanilla RNN (ê¸°ë³¸ì ì¸ RNN)

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%203.png)

- weightê°’ì€ ë˜‘ê°™ì€ ê²ƒì„ ì‚¬ìš©

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%204.png)

- ì•ŒíŒŒë²³ì´ ì£¼ì–´ ì¡Œì„ ë•Œ ê·¸ ë‹¤ìŒì— ì˜¬ ì•ŒíŒŒë²³ì„ ì˜ˆì¸¡

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%205.png)

- ê°ê°ì˜ ì•ŒíŒŒë²³ì„ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜ í›„
- ê°€ì¤‘ì¹˜ë¥¼ ê³±í•œë‹¤. ì´ ë•Œ ì „ layerì—ì„œ êµ¬í•˜ì˜€ë˜ h_scoreê°’ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. (ë‹¨. ì²« ë²ˆì§¸ layerëŠ” ì „ì— êµ¬í•œ layerì˜ h_scoreê°’ì´ ì—†ìœ¼ë¯€ë¡œ h_scoreì„ 0ìœ¼ë¡œ ë‘ì–´ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤.)
- hidden layerì´í›„ì—ëŠ” CNNê³¼ ê°™ì´ FC layerë¥¼ ì‚¬ìš©í•˜ì—¬ scoreë¥¼ êµ¬í•œë‹¤.

  

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%206.png)

- RNN í™œìš© ì˜ˆì‹œ

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%207.png)

- RNNì„ ë” ë³µì¡í•œ layerë¥¼ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤.

Vanillaë³´ë‹¤ ëª‡ ê°œì˜ ë” ì¢‹ì€ ëª¨ë¸

- Long Short Term Memory (LSTM)
- GRU

### RNN ëª¨ë¸ êµ¬í˜„ (LSTM)

```python
vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)
word_vector_dim = 4  # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ìˆ˜ì…ë‹ˆë‹¤. 

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))
model.add(tf.keras.layers.LSTM(8))   # ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” RNNì¸ LSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë•Œ LSTM state ë²¡í„°ì˜ ì°¨ì›ìˆ˜ëŠ” 8ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. (ë³€ê²½ ê°€ëŠ¥)
model.add(tf.keras.layers.Dense(8, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.

model.summary()
'''
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_3 (Embedding)      (None, None, 4)           40        
_________________________________________________________________
lstm (LSTM)                  (None, 8)                 416       
_________________________________________________________________
dense (Dense)                (None, 8)                 72        
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 9         
=================================================================
Total params: 537
Trainable params: 537
Non-trainable params: 0
_________________________________________________________________
'''
```

# ê¼­ RNNì´ì–´ì•¼ í• ê¹Œ?

í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ RNNì´ ì•„ë‹Œë¼ **`1-D Convolution Neural Network(1-D CNN)`** ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. 1-D CNNì—ì„œëŠ” ë­Šì•„ ì „ì²´ë¥¼ í•œêº¼ë²ˆì— í•œ ë°©í–¥ìœ¼ë¡œ ê¸¸ì´ 7ì§œë¦¬ í”¼í„°ë¡œ ìŠ¤ìºë‹ í•˜ë©´ì„œ 7ë‹¨ì–´ì´ë‚´ì—ì„œ ë°œê²¬ë˜ëŠ” íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ê·¸ê²ƒìœ¼ë¡œ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

íŠ¹ì§•

- RNN ëª»ì§€ì•Šì€ íš¨ìœ¨ì„ ë³´ì—¬ì¤€ë‹¤.
- CNNê³„ì—´ì€ RNNê³„ì—´ë³´ë‹¤ ë³‘ì—´ì²˜ë¦¬ê°€ íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì— í•™ìŠµ ì†ë„ë„ í›¨ì”¬ ë¹ ë¥´ë‹¤.

### CNN ëª¨ë¸ êµ¬í˜„ 1

```python
vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)
word_vector_dim = 4   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. 

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))
model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))
model.add(tf.keras.layers.MaxPooling1D(5))
model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))
model.add(tf.keras.layers.GlobalMaxPooling1D())
model.add(tf.keras.layers.Dense(8, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.

model.summary()
'''
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, None, 4)           40        
_________________________________________________________________
conv1d (Conv1D)              (None, None, 16)          464       
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, None, 16)          0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, None, 16)          1808      
_________________________________________________________________
global_max_pooling1d (Global (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 9         
=================================================================
Total params: 2,457
Trainable params: 2,457
Non-trainable params: 0
_________________________________________________________________
'''
```

### CNN ëª¨ë¸ êµ¬í˜„ 2

```python
vocab_size = 10  # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10ê°œì˜ ë‹¨ì–´)
word_vector_dim = 4   # ë‹¨ì–´ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. 

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))
model.add(tf.keras.layers.GlobalMaxPooling1D())
model.add(tf.keras.layers.Dense(8, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # ìµœì¢… ì¶œë ¥ì€ ê¸ì •/ë¶€ì •ì„ ë‚˜íƒ€ë‚´ëŠ” 1dim ì…ë‹ˆë‹¤.

model.summary()
'''
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, None, 4)           40        
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 4)                 0         
_________________________________________________________________
dense_4 (Dense)              (None, 8)                 40        
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 9         
=================================================================
Total params: 89
Trainable params: 89
Non-trainable params: 0
'''
```

ë‹¤ë¥¸ ë°©ë²•

- 1-D CNNê³¼ RNNì„ ì„ì–´ ì‚¬ìš©
- FFN(FeedForwaed Network)
- Transformer ì‚¬ìš©

# IMDB ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„

### IMDB ë°ì´í„°ì…‹ ë¶„ì„

IMDb Large Movie Dataset

- 50000ê°œì˜ ì˜ì–´ë¡œ ì‘ì„±ëœ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ (train: 25000, test: 25000)
- ê¸ì •ì€ 1, ë¶€ì •ì€ 0ì˜ ë¼ë²¨

### ë°ì´í„° í™•ì¸

```python
imdb = tf.keras.datasets.imdb

# IMDb ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ 
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)
print("í›ˆë ¨ ìƒ˜í”Œ ê°œìˆ˜: {}, í…ŒìŠ¤íŠ¸ ê°œìˆ˜: {}".format(len(x_train), len(x_test)))
'''
í›ˆë ¨ ìƒ˜í”Œ ê°œìˆ˜: 25000, í…ŒìŠ¤íŠ¸ ê°œìˆ˜: 25000
'''
```

```python
# ìˆ«ìë¡œ encodeëœ í…ìŠ¤íŠ¸ ë°ì´í„°

print(x_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°
print('ë¼ë²¨: ', y_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°ì˜ ë¼ë²¨
print('1ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´: ', len(x_train[0]))
print('2ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´: ', len(x_train[1]))
'''
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
ë¼ë²¨:  1
1ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´:  218
2ë²ˆì§¸ ë¦¬ë·° ë¬¸ì¥ ê¸¸ì´:  189
'''
```

```python
# encodeì— ì‚¬ìš©í•œ ë”•ì…”ë„ˆë¦¬

word_to_index = imdb.get_word_index()
index_to_word = {index:word for word, index in word_to_index.items()}
print(index_to_word[1])     # 'the' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. 
print(word_to_index['the'])  # 1 ì´ ì¶œë ¥ë©ë‹ˆë‹¤.
'''
the
1
'''
```

```python
# ì‹¤ì œë¡œëŠ” 3ê°œì”© ì¸ë±ìŠ¤ê°€ ë°€ë ¤ìˆì–´ì„œ ì•ì— 3ê°œë¥¼ ì¶”ê°€í•˜ì—¬ ì¸ë±ìŠ¤ ë§ì¶”ê¸°

#ì‹¤ì œ ì¸ì½”ë”© ì¸ë±ìŠ¤ëŠ” ì œê³µëœ word_to_indexì—ì„œ index ê¸°ì¤€ìœ¼ë¡œ 3ì”© ë’¤ë¡œ ë°€ë ¤ ìˆìŠµë‹ˆë‹¤.  
word_to_index = {k:(v+3) for k,v in word_to_index.items()}

# ì²˜ìŒ ëª‡ ê°œ ì¸ë±ìŠ¤ëŠ” ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤
word_to_index["<PAD>"] = 0
word_to_index["<BOS>"] = 1
word_to_index["<UNK>"] = 2  # unknown
word_to_index["<UNUSED>"] = 3

index_to_word = {index:word for word, index in word_to_index.items()}

print(index_to_word[1])     # '<BOS>' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. 
print(word_to_index['the'])  # 4 ì´ ì¶œë ¥ë©ë‹ˆë‹¤. 
print(index_to_word[4])     # 'the' ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.
'''
<BOS>
4
the
'''
```

```python
# decodeë™ì‘ í™•ì¸
print(get_decoded_sentence(x_train[0], index_to_word))
print('ë¼ë²¨: ', y_train[0])  # 1ë²ˆì§¸ ë¦¬ë·°ë°ì´í„°ì˜ ë¼ë²¨
'''
this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
ë¼ë²¨:  1
'''
```

```python
total_data_text = list(x_train) + list(x_test)
# í…ìŠ¤íŠ¸ë°ì´í„° ë¬¸ì¥ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„
num_tokens = [len(tokens) for tokens in total_data_text]
num_tokens = np.array(num_tokens)
# ë¬¸ì¥ê¸¸ì´ì˜ í‰ê· ê°’, ìµœëŒ€ê°’, í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•´ ë³¸ë‹¤. 
print('ë¬¸ì¥ê¸¸ì´ í‰ê·  : ', np.mean(num_tokens))
print('ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ : ', np.max(num_tokens))
print('ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ : ', np.std(num_tokens))

# ì˜ˆë¥¼ë“¤ì–´, ìµœëŒ€ ê¸¸ì´ë¥¼ (í‰ê·  + 2*í‘œì¤€í¸ì°¨)ë¡œ í•œë‹¤ë©´,  
max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)
maxlen = int(max_tokens)
print('pad_sequences maxlen : ', maxlen)
print('ì „ì²´ ë¬¸ì¥ì˜ {}%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))
'''
ë¬¸ì¥ê¸¸ì´ í‰ê·  :  234.75892
ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ :  2494
ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ :  172.91149458735703
pad_sequences maxlen :  580
ì „ì²´ ë¬¸ì¥ì˜ 0.94536%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤.
'''
```

### train, test ë°ì´í„° ìƒì„±

```python
# padding ë°©ì‹ ê²°ì •
x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,
                                                        value=word_to_index["<PAD>"],
                                                        padding='post', # í˜¹ì€ 'pre'
                                                        maxlen=maxlen)

x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,
                                                       value=word_to_index["<PAD>"],
                                                       padding='post', # í˜¹ì€ 'pre'
                                                       maxlen=maxlen)

print(x_train.shape)
'''
(25000, 580)
'''
```

<aside>
ğŸ’¡ **Q9. RNN í™œìš© ì‹œ pad_sequencesì˜ padding ë°©ì‹ì€ 'post'ì™€ 'pre' ì¤‘ ì–´ëŠ ê²ƒì´ ìœ ë¦¬í• ê¹Œìš”? ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?

A. RNNì€ ì…ë ¥ë°ì´í„°ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´, ê°€ì¥ ë§ˆì§€ë§‰ ì…ë ¥ì´ ìµœì¢… state ê°’ì— ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë¯¸ì¹˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë§ˆì§€ë§‰ ì…ë ¥ì´ ë¬´ì˜ë¯¸í•œ paddingìœ¼ë¡œ ì±„ì›Œì§€ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ 'pre'ê°€ í›¨ì”¬ ìœ ë¦¬í•˜ë©°, 10% ì´ìƒì˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì´ê²Œ ë©ë‹ˆë‹¤.**

</aside>

### ëª¨ë¸ ì„¤ê³„

```python
vocab_size = 10000    # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10,000ê°œì˜ ë‹¨ì–´)
word_vector_dim = 16  # ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜ (ë³€ê²½ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°)

# model ì„¤ê³„ - ë”¥ëŸ¬ë‹ ëª¨ë¸ ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•´ ì£¼ì„¸ìš”.
model = tf.keras.Sequential()
# [[YOUR CODE]]
model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))
model.add(tf.keras.layers.LSTM(8))   # ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” RNNì¸ LSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë•Œ LSTM state ë²¡í„°ì˜ ì°¨ì›ìˆ˜ëŠ” 8ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. (ë³€ê²½ ê°€ëŠ¥)
model.add(tf.keras.layers.Dense(8, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.summary()
'''
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (None, None, 16)          160000    
_________________________________________________________________
lstm_1 (LSTM)                (None, 8)                 800       
_________________________________________________________________
dense_6 (Dense)              (None, 8)                 72        
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 9         
=================================================================
Total params: 160,881
Trainable params: 160,881
Non-trainable params: 0
_________________________________________________________________
'''
```

### validation set ë¶„ë¦¬

```python
# validation set 10000ê±´ ë¶„ë¦¬
x_val = x_train[:10000]   
y_val = y_train[:10000]

# validation setì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ 15000ê±´
partial_x_train = x_train[10000:]  
partial_y_train = y_train[10000:]

print(partial_x_train.shape)
print(partial_y_train.shape)
'''
(15000, 580)
(15000,)
'''
```

### ëª¨ë¸ í•™ìŠµ

```python
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
              
epochs=20  # ëª‡ epochë¥¼ í›ˆë ¨í•˜ë©´ ì¢‹ì„ì§€ ê²°ê³¼ë¥¼ ë³´ë©´ì„œ ë°”ê¾¸ì–´ ë´…ì‹œë‹¤. 

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=epochs,
                    batch_size=512,
                    validation_data=(x_val, y_val),
                    verbose=1)
'''
loss: 0.6897 - accuracy: 0.5261 - val_loss: 0.6937 - val_accuracy: 0.5048
'''
```

### ëª¨ë¸ í‰ê°€

```python
results = model.evaluate(x_test,  y_test, verbose=2)

print(results)
'''
782/782 - 5s - loss: 0.6933 - accuracy: 0.5075
[0.6932967901229858, 0.5074800252914429]
'''
```

### í‰ê°€ ì‹œê°í™”

```python
history_dict = history.history
print(history_dict.keys()) # epochì— ë”°ë¥¸ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³¼ ìˆ˜ ìˆëŠ” í•­ëª©ë“¤
'''
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
'''

import matplotlib.pyplot as plt

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo"ëŠ” "íŒŒë€ìƒ‰ ì "ì…ë‹ˆë‹¤
plt.plot(epochs, loss, 'bo', label='Training loss')
# bëŠ” "íŒŒë€ ì‹¤ì„ "ì…ë‹ˆë‹¤
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%208.png)

```python
plt.clf()   # ê·¸ë¦¼ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()
```

![Untitled](Exploration%206%20%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AA%E1%84%85%E1%85%B5%E1%84%87%E1%85%B2%20%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%80%E1%85%A1%E1%86%B7%E1%84%89%E1%85%A5%E1%86%BC%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5%20d6b3bef60fa14ebeb44639485ffc9504/Untitled%209.png)

### Word Embedding

<aside>
ğŸ’¡ **ì›Œë“œ ì„ë² ë”©(word embedding)**
ë¼ë²¨ë§ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„ì˜ ë¹„ìš©ì„ ì ˆê°í•˜ë©´ì„œ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ìì—°ì–´ì²˜ë¦¬ ê¸°ë²• (ë‹¨ì–´ì˜ íŠ¹ì„±ì„ ì €ì°¨ì› ë²¡í„°ê°’ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ)

</aside>

ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©í•œ Embedding ë ˆì´ì–´

- (ìš°ë¦¬ê°€ ê°€ì§„ ì‚¬ì „ì˜ ë‹¨ì–´ ê°œìˆ˜) x (ì›Œë“œ ë²¡í„° ì‚¬ì´ì¦ˆ)ë§Œí¼ì˜ í¬ê¸°

```python
# Embedding layerì˜ í•™ìŠµëœ wordvector í™•ì¸
embedding_layer = model.layers[0]
weights = embedding_layer.get_weights()[0]
print(weights.shape)    # shape: (vocab_size, embedding_dim)
'''
(10000, 16)
'''
```

```python
# í•™ìŠµí•œ Embedding íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì¼ì— ì¨ì„œ ì €ì¥í•©ë‹ˆë‹¤. 
word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'
f = open(word2vec_file_path, 'w')
f.write('{} {}\n'.format(vocab_size-4, word_vector_dim))  # ëª‡ê°œì˜ ë²¡í„°ë¥¼ ì–¼ë§ˆ ì‚¬ì´ì¦ˆë¡œ ê¸°ì¬í• ì§€ íƒ€ì´í‹€ì„ ì”ë‹ˆë‹¤.

# ë‹¨ì–´ ê°œìˆ˜(ì—ì„œ íŠ¹ìˆ˜ë¬¸ì 4ê°œëŠ” ì œì™¸í•˜ê³ )ë§Œí¼ì˜ ì›Œë“œ ë²¡í„°ë¥¼ íŒŒì¼ì— ê¸°ë¡í•©ë‹ˆë‹¤. 
vectors = model.get_weights()[0]
for i in range(4,vocab_size):
    f.write('{} {}\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))
f.close()
```

### gensim package

gensimíŒ¨í‚¤ì§€ë¡œ ì €ì¥í•œ ì„ë² ë”© íŒŒë¼ë¯¸í„°ë¥¼ ì½ì–´ì„œ wordvectorë¡œ ì‚¬ìš©

```python
from gensim.models.keyedvectors import Word2VecKeyedVectors

word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)
vector = word_vectors['computer']
vector
'''
array([-0.05156684, -0.05027233,  0.03619769, -0.00985204,  0.01735285,
        0.02981805,  0.0511942 , -0.01146023, -0.01958035, -0.00826631,
        0.02113515,  0.00083194,  0.03631599,  0.00501615,  0.04324353,
       -0.00967919], dtype=float32)
'''
```

gensimë¥¼ ì´ìš©í•˜ì—¬ íŠ¹ì • ë‹¨ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´, ìœ ì‚¬ë„ í™•ì¸

```python
word_vectors.similar_by_word("love")
'''
[('exceptional', 0.9821829795837402),
 ('step', 0.9752857089042664),
 ('easier', 0.9750139117240906),
 ('heath', 0.9734551310539246),
 ('solid', 0.9727447628974915),
 ('revealed', 0.9727013111114502),
 ('halloween', 0.97214674949646),
 ('futuristic', 0.9710436463356018),
 ('9', 0.9704533815383911),
 ('terrific', 0.9700677394866943)]
'''
```

ìš°ë¦¬ê°€ ë‹¤ë£¬ ì •ë„ì˜ í›ˆë ¨ ë°ì´í„°ë¡œëŠ” ì›Œë“œ ë²¡í„°ë¥¼ ì •êµí•˜ê²Œ í•™ìŠµì‹œí‚¤ê¸° ì–´ë µë‹¤.

### Word2Vec ì‚¬ìš©

- googleì„¸ì–´ ì œê³µí•˜ëŠ” ì‚¬ì „í•™ìŠµëœ ì›Œë“œ ì„ë² ë”© ëª¨ë¸
- 1ì–µ ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ Google News datasetì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ
- ì´ 300ë§Œ ê°œì˜ ë‹¨ì–´ë¥¼ ê°ê° 300ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„

```python
# 300ë§Œê°œ ì¤‘ 100ë§Œê°œë§Œ ë¡œë”©

from gensim.models import KeyedVectors

word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/GoogleNews-vectors-negative300.bin.gz'
word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)
vector = word2vec['computer']
vector     # ë¬´ë ¤ 300dimì˜ ì›Œë“œ ë²¡í„°ì…ë‹ˆë‹¤.
```

Word2Vecë¥¼ ê°€ì§€ê³  ìœ ì‚¬ë„ í™•ì¸

```python
# ë©”ëª¨ë¦¬ë¥¼ ë‹¤ì†Œ ë§ì´ ì†Œë¹„í•˜ëŠ” ì‘ì—…ì´ë‹ˆ ìœ ì˜í•´ ì£¼ì„¸ìš”.
word2vec.similar_by_word("love")
'''
[('loved', 0.6907791495323181),
 ('adore', 0.6816873550415039),
 ('loves', 0.661863386631012),
 ('passion', 0.6100708842277527),
 ('hate', 0.600395679473877),
 ('loving', 0.5886635780334473),
 ('Ilove', 0.5702950954437256),
 ('affection', 0.5664337873458862),
 ('undying_love', 0.5547304749488831),
 ('absolutely_adore', 0.5536840558052063)]
'''
```

í•™ìŠµí–ˆë˜ ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ë¥¼ Word2Vecì˜ ê²ƒìœ¼ë¡œ êµì²´í•˜ì—¬ ë‹¤ì‹œ í•™ìŠµ

```python
vocab_size = 10000    # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10,000ê°œì˜ ë‹¨ì–´)
word_vector_dim = 300  # ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜
embedding_matrix = np.random.rand(vocab_size, word_vector_dim)

# embedding_matrixì— Word2Vec ì›Œë“œ ë²¡í„°ë¥¼ ë‹¨ì–´ í•˜ë‚˜ì”©ë§ˆë‹¤ ì°¨ë¡€ì°¨ë¡€ ì¹´í”¼í•œë‹¤.
for i in range(4,vocab_size):
    if index_to_word[i] in word2vec:
        embedding_matrix[i] = word2vec[index_to_word[i]]
```

```python
from tensorflow.keras.initializers import Constant

vocab_size = 10000    # ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°ì…ë‹ˆë‹¤(10,000ê°œì˜ ë‹¨ì–´)
word_vector_dim = 300  # ì›Œë“œ ë²¡í„°ì˜ ì°¨ì› ìˆ˜ 

# ëª¨ë¸ êµ¬ì„±
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size, 
                                 word_vector_dim, 
                                 embeddings_initializer=Constant(embedding_matrix),  # ì¹´í”¼í•œ ì„ë² ë”©ì„ ì—¬ê¸°ì„œ í™œìš©
                                 input_length=maxlen, 
                                 trainable=True))   # trainableì„ Trueë¡œ ì£¼ë©´ Fine-tuning
model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))
model.add(tf.keras.layers.MaxPooling1D(5))
model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))
model.add(tf.keras.layers.GlobalMaxPooling1D())
model.add(tf.keras.layers.Dense(8, activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid')) 

model.summary()
'''
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, 580, 300)          3000000   
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 574, 16)           33616     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 114, 16)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 108, 16)           1808      
_________________________________________________________________
global_max_pooling1d_2 (Glob (None, 16)                0         
_________________________________________________________________
dense_8 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_9 (Dense)              (None, 1)                 9         
=================================================================
Total params: 3,035,569
Trainable params: 3,035,569
Non-trainable params: 0
_________________________________________________________________
'''
```

í•™ìŠµ

```python
# í•™ìŠµì˜ ì§„í–‰
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
              
epochs=20  # ëª‡ epochë¥¼ í›ˆë ¨í•˜ë©´ ì¢‹ì„ì§€ ê²°ê³¼ë¥¼ ë³´ë©´ì„œ ë°”ê¾¸ì–´ ë´…ì‹œë‹¤. 

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=epochs,
                    batch_size=512,
                    validation_data=(x_val, y_val),
                    verbose=1)
```

í‰ê°€

```python
# í…ŒìŠ¤íŠ¸ì…‹ì„ í†µí•œ ëª¨ë¸ í‰ê°€
results = model.evaluate(x_test,  y_test, verbose=2)

print(results)
'''
782/782 - 2s - loss: 0.5900 - accuracy: 0.8573
[0.5900180339813232, 0.8572800159454346]
'''
```