# Exploration 4 ì‘ì‚¬ê°€ ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°

# 1. **ì‹œí€€ìŠ¤? ìŠ¤í€€ìŠ¤!I**

íŒŒì´ì¬ì—ì„œì˜ ì‹œí€€ìŠ¤ ìë£Œí˜•

> ê°’ì´ ì—°ì†ì ìœ¼ë¡œ ì´ì–´ì§„ ìë£Œí˜•ë“¤ì„ ì´ì¹­í•˜ì—¬ â€˜ì‹œí€€ìŠ¤ ìë£Œí˜•(sequence type)â€™ì´ë¼ê³  í•œë‹¤.
> 

ì¸ê³µì§€ëŠ¥ì´ ì˜ˆì¸¡ì„ í•˜ê¸°ìœ„í•´ì„œëŠ” **ì–´ëŠ ì •ë„ëŠ” ì—°ê´€ì„±ì´ ìˆì–´ì•¼í•œë‹¤.**

# 2. **ë‹¤ìŒ amì„ ì“°ë©´ ë°˜ ì´ìƒì€ ë§ë”ë¼**

### **ìˆœí™˜ì‹ ê²½ë§(RNN)**

![Untitled](Exploration%204%20%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%89%E1%85%A1%E1%84%80%E1%85%A1%20%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%B5%E1%84%82%E1%85%B3%E1%86%BC%20%E1%84%86%E1%85%A1%E1%86%AB%E1%84%83%E1%85%B3%E1%86%AF%E1%84%80%E1%85%B5%20969cc7125e2b4138b2fbdff89534e2a7/Untitled.png)

ìš°ë¦¬ëŠ” â€˜ë‚˜ëŠ” ë°¥ì„â€™ì´ë¼ëŠ” inputì„ ê°€ì§€ê³  â€˜ë¨¹ì—ˆë‹¤â€™ë¼ëŠ” outputì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë©´ â€˜ë‚˜ëŠ”â€™ ì´ë¼ëŠ” outputì„ ë‚´ë ¤ë©´ ì–´ë–»ê²Œ í• ê¹Œ?

ìˆœí™˜ì‹ ê²½ë§ì—ì„œëŠ” outputì„ ë‹¤ì‹œ inputìœ¼ë¡œ ì‚¬ìš©í•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. ì´ëŸ¬í•œ ìˆœí™˜ì  êµ¬ì¡°ì—ì„œ ì‹œì‘ê³¼ ëì€ <start>, <end>ë¼ëŠ” ê²ƒì„ ì‚¬ìš©í•´ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤.

```python
sentence = " ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤ "

source_sentence = "<start>" + sentence
target_sentence = sentence + "<end>"

print("Source ë¬¸ì¥:", source_sentence)
print("Target ë¬¸ì¥:", target_sentence)
'''
>>>
Source ë¬¸ì¥: <start> ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤ 
Target ë¬¸ì¥:  ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤ <end>
'''
```

### ì–¸ì–´ëª¨ë¸ (Language Model)

<aside>
ğŸ’¡ **ì–¸ì–´ ëª¨ë¸ì´ë€?** 
*n*âˆ’1ê°œì˜ ë‹¨ì–´ ì‹œí€€ìŠ¤w1,...wn-1ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, *n*ë²ˆì§¸ ë‹¨ì–´wnìœ¼ë¡œ ë¬´ì—‡ì´ ì˜¬ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í™•ë¥  ëª¨ë¸

</aside>

# **ì‹¤ìŠµ**

## 1. ë°ì´í„° ì „ì²˜ë¦¬

### ë°ì´í„° ë‹¤ìš´ë¡œë“œ

```python
import os, re 
import numpy as np
import tensorflow as tf

# íŒŒì¼ì„ ì½ê¸°ëª¨ë“œë¡œ ì—´ê³ 
# ë¼ì¸ ë‹¨ìœ„ë¡œ ëŠì–´ì„œ list í˜•íƒœë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'
with open(file_path, "r") as f:
    raw_corpus = f.read().splitlines()

# ì•ì—ì„œë¶€í„° 10ë¼ì¸ë§Œ í™”ë©´ì— ì¶œë ¥í•´ ë³¼ê¹Œìš”?
print(raw_corpus[:9])
```

ë°ì´í„°ëŠ” ì™„ë²½í•œ ì—°ê·¹ ëŒ€ë³¸ì´ë‹¤. í•˜ì§€ë§Œ í•„ìš”ì—†ëŠ” ê³µë°±ì •ë³´ê°€ í¬í•¨ë˜ì—ˆë‹¤.

![Untitled](Exploration%204%20%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%89%E1%85%A1%E1%84%80%E1%85%A1%20%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%B5%E1%84%82%E1%85%B3%E1%86%BC%20%E1%84%86%E1%85%A1%E1%86%AB%E1%84%83%E1%85%B3%E1%86%AF%E1%84%80%E1%85%B5%20969cc7125e2b4138b2fbdff89534e2a7/Untitled%201.png)

```python
for idx, sentence in enumerate(raw_corpus):
    if len(sentence) == 0: continue   # ê¸¸ì´ê°€ 0ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.
    if sentence[-1] == ":": continue  # ë¬¸ì¥ì˜ ëì´ : ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.

    if idx > 9: break   # ì¼ë‹¨ ë¬¸ì¥ 10ê°œë§Œ í™•ì¸í•´ ë³¼ ê²ë‹ˆë‹¤.
        
    print(sentence)
```

### í† í°í™”

ì´ì œ ë¬¸ì¥ì„ ì¼ì •í•œ ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°œì•¼ í•˜ëŠ” **í† í°í™”** ì‘ì—…ì„ í•´ì•¼í•œë‹¤.

ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•  ê²½ìš° ëª‡ ê°€ì§€ì˜ ë¬¸ì œì ì´ ìˆë‹¤.

1. Hi, my name is John. *("Hi," "my", ..., "john." ìœ¼ë¡œ ë¶„ë¦¬ë¨) - ë¬¸ì¥ë¶€í˜¸
2. First, open the first chapter. *(Firstì™€ firstë¥¼ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹) - ëŒ€ì†Œë¬¸ì 
3. He is a ten-year-old boy. *(ten-year-oldë¥¼ í•œ ë‹¨ì–´ë¡œ ì¸ì‹) - íŠ¹ìˆ˜ë¬¸ì

ì´ëŸ¬í•œ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ì •ê·œí‘œí˜„ì‹(Regex)ì„ ì´ìš©í•œ í•„í„°ë§ì´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤.

```python
# ì…ë ¥ëœ ë¬¸ì¥ì„
#     1. ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤
#     2. íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ë„£ê³ 
#     3. ì—¬ëŸ¬ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤
#     4. a-zA-Z?.!,Â¿ê°€ ì•„ë‹Œ ëª¨ë“  ë¬¸ìë¥¼ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤
#     5. ë‹¤ì‹œ ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤
#     6. ë¬¸ì¥ ì‹œì‘ì—ëŠ” <start>, ëì—ëŠ” <end>ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
# ì´ ìˆœì„œë¡œ ì²˜ë¦¬í•´ì£¼ë©´ ë¬¸ì œê°€ ë˜ëŠ” ìƒí™©ì„ ë°©ì§€í•  ìˆ˜ ìˆê² ë„¤ìš”!
def preprocess_sentence(sentence):
    sentence = sentence.lower().strip() # 1
    sentence = re.sub(r"([?.!,Â¿])", r" \1 ", sentence) # 2
    sentence = re.sub(r'[" "]+', " ", sentence) # 3
    sentence = re.sub(r"[^a-zA-Z?.!,Â¿]+", " ", sentence) # 4
    sentence = sentence.strip() # 5
    sentence = '<start> ' + sentence + ' <end>' # 6
    return sentence

# ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.
print(preprocess_sentence("This @_is ;;;sample        sentence."))
'''
>>>
<start> this is sample sentence . <end>
'''
```

ì´ìƒí•˜ê³  ë”ëŸ¬ìš´ ë¬¸ì¥ì„ ë„£ì–´ë„ ê¹”ë”í•˜ê²Œ í‘œí˜„ëœë‹¤.

ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ëª¨ë¸ì˜ ì…ë ¥ì´ ë˜ëŠ” ë¬¸ì¥ì„ ì†ŒìŠ¤ë¬¸ì¥(Source Sentence), ì •ë‹´ ì—­í• ì„ í•˜í•˜ëŠ” ì¶œë ¥ë¬¸ì¥ì„ íƒ€ê²Ÿ ë¬¸ì¥(Target Sentence)ì´ë¼ê³  í•œë‹¤. ìœ„ì—ì„œ ë§Œë“  ì •ì œí™” í•¨ìˆ˜ë¥¼ í†µí•´ í† í°í™”ë¥¼ ì§„í–‰í•œ í›„ ë ë‹¨ì–´ <end>ë¥¼ ì—†ì• ë©´ ì†ŒìŠ¤ë¬¸ì¥, ì²« ë‹¨ì–´ <start>ë¥¼ ì—†ì• ë©´ íƒ€ê²Ÿ ë¬¸ì¥ì´ ëœë‹¤.

 

```python
# ì—¬ê¸°ì— ì •ì œëœ ë¬¸ì¥ì„ ëª¨ì„ê²ë‹ˆë‹¤
corpus = []

for sentence in raw_corpus:
    # ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤
    if len(sentence) == 0: continue
    if sentence[-1] == ":": continue
    
    # ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”
    preprocessed_sentence = preprocess_sentence(sentence)
    corpus.append(preprocessed_sentence)
        
# ì •ì œëœ ê²°ê³¼ë¥¼ 10ê°œë§Œ í™•ì¸í•´ë³´ì£ 
corpus[:10]
'''
>>>
['<start> before we proceed any further , hear me speak . <end>',
 '<start> speak , speak . <end>',
 '<start> you are all resolved rather to die than to famish ? <end>',
 '<start> resolved . resolved . <end>',
 '<start> first , you know caius marcius is chief enemy to the people . <end>',
 '<start> we know t , we know t . <end>',
 '<start> let us kill him , and we ll have corn at our own price . <end>',
 '<start> is t a verdict ? <end>',
 '<start> no more talking on t let it be done away , away ! <end>',
 '<start> one word , good citizens . <end>']
'''
```

ìš°ë¦¬ê°€ ì˜ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë“¯ ì»´í“¨í„°ë„ ì–¸ì–´ê°€ ë“¤ì–´ì˜¤ë©´ ìˆ«ìë¡œ ë²ˆì—­í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. 

tf.keras.preprocessing.text.TokenizeríŒ¨í‚¤ì§€ëŠ” ì •ì¬ëœ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ë‹¨ì–´ì‚¬ì „ì„ ë§Œë“¤ì–´ ì£¼ë©°, ë”©í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜ê¹Œì§€ í•œ ë²ˆì— í•´ì¤€ë‹¤. ì´ê³¼ì •ì„ **ë²¡í„°í™”(vectorize)**ë¼ í•˜ë©°, ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ **í…ì„œ(tensor)**ë¼ê³  í•œë‹¤.

```python
# í† í°í™” í•  ë•Œ í…ì„œí”Œë¡œìš°ì˜ Tokenizerì™€ pad_sequencesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
# ë” ì˜ ì•Œê¸° ìœ„í•´ ì•„ë˜ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤
# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer
# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences
def tokenize(corpus):
    # 7000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizerë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤
    # ìš°ë¦¬ëŠ” ì´ë¯¸ ë¬¸ì¥ì„ ì •ì œí–ˆìœ¼ë‹ˆ filtersê°€ í•„ìš”ì—†ì–´ìš”
    # 7000ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ëª»í•œ ë‹¨ì–´ëŠ” '<unk>'ë¡œ ë°”ê¿€ê±°ì—ìš”
    tokenizer = tf.keras.preprocessing.text.Tokenizer( 
        num_words=7000, 
        filters=' ',
        oov_token="<unk>"
    )
    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤
    tokenizer.fit_on_texts(corpus)
    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤
    tensor = tokenizer.texts_to_sequences(corpus)   
    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤
    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.
    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  
    
    print(tensor,tokenizer)
    return tensor, tokenizer

tensor, tokenizer = tokenize(corpus)
```

```python
# ìƒì„±ëœ í…ì„œ ë°ì´í„°ë¥¼ 3ë²ˆì§¸ í–‰, 10ë²ˆì§¸ ì—´ê¹Œì§€ë§Œ ì¶œë ¥
print(tensor[:3, :10])
'''
>>>
[[   2  143   40  933  140  591    4  124   24  110]
 [   2  110    4  110    5    3    0    0    0    0]
 [   2   11   50   43 1201  316    9  201   74    9]]
'''
```

```python
# tokenizerì— êµ¬ì¶•ëœ ë‹¨ì–´ ì‚¬ì „ í™•ì¸
for idx in tokenizer.index_word:
    print(idx, ":", tokenizer.index_word[idx])

    if idx >= 10: break
'''
>>>
1 : <unk>
2 : <start>
3 : <end>
4 : ,
5 : .
6 : the
7 : and
8 : i
9 : to
10 : of
'''
```

ìœ„ì— í…ì„œ ë°ì´í„°ì˜ ì‹œì‘ì´ 2ì¸ ì´ë¥˜ëŠ” tokenizerì— êµ¬ì¶•ëœ ë‹¨ì–´ì‚¬ì „ ì† <start>ì˜ ì¸ë±ìŠ¤ê°€ 2ì´ê¸° ë•Œë¬¸ì´ë‹¤.

### ì†ŒìŠ¤ì™€ íƒ€ê²Ÿë¶„ë¦¬

```python
# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤
# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
src_input = tensor[:, :-1]  
# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
tgt_input = tensor[:, 1:]    

print(src_input[0])
print(tgt_input[0])
```

### tf.data.Datasetê°ì²´ ìƒì„±

ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ numpy arrayí˜•íƒœì˜ ë°ì´í„° ì…‹ì„ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ RNNì—ì„œëŠ” tf.data.Datasetê°ì²´ë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤.

```python
BUFFER_SIZE = len(src_input)
BATCH_SIZE = 256
steps_per_epoch = len(src_input) // BATCH_SIZE

 # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 7000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ
VOCAB_SIZE = tokenizer.num_words + 1   

# ì¤€ë¹„í•œ ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤
# ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”
# ìì„¸íˆ ì•Œì•„ë‘˜ìˆ˜ë¡ ë„ì›€ì´ ë§ì´ ë˜ëŠ” ì¤‘ìš”í•œ ë¬¸ì„œì…ë‹ˆë‹¤
# https://www.tensorflow.org/api_docs/python/tf/data/Dataset
dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))
dataset = dataset.shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)
dataset
'''
>>>
<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>
'''
```

ë°ì´í„°ì…‹ ìƒì„±ê³¼ì • ì •ë¦¬

- **ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ corpus ìƒì„±**
- **`tf.keras.preprocessing.text.Tokenizer`ë¥¼ ì´ìš©í•´ corpusë¥¼ í…ì„œë¡œ ë³€í™˜**
- **`tf.data.Dataset.from_tensor_slices()`ë¥¼ ì´ìš©í•´ corpus í…ì„œë¥¼`tf.data.Dataset`ê°ì²´ë¡œ ë³€í™˜**

## 2. í•™ìŠµ ì‹œí‚¤ê¸°

ë§Œë“¤ ëª¨ë¸ì˜ êµ¬ì¡°

![Untitled](Exploration%204%20%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%89%E1%85%A1%E1%84%80%E1%85%A1%20%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%B5%E1%84%82%E1%85%B3%E1%86%BC%20%E1%84%86%E1%85%A1%E1%86%AB%E1%84%83%E1%85%B3%E1%86%AF%E1%84%80%E1%85%B5%20969cc7125e2b4138b2fbdff89534e2a7/Untitled%202.png)

ìš°ë¦¬ê°€ ë§Œë“¤ ëª¨ë¸ì€ tf.keras.Modelì„ Subclassingí•˜ëŠ” ë°©ì‹ì´ë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ìš°ë¦¬ê°€ ë§Œë“¤ ëª¨ë¸ì—ëŠ” 1ê°œì˜ Embedding ë ˆì´ì–´, 2ê°œì˜ LSTM ë ˆì´ì–´, 1ê°œì˜ Dense ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.

### ëª¨ë¸ ìƒì„±

```python
class TextGenerator(tf.keras.Model):
    def __init__(self, vocab_size, embedding_size, hidden_size):
        super().__init__()
        
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)
        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)
        self.linear = tf.keras.layers.Dense(vocab_size)
        
    def call(self, x):
        out = self.embedding(x)
        out = self.rnn_1(out)
        out = self.rnn_2(out)
        out = self.linear(out)
        
        return out
    
embedding_size = 256
hidden_size = 1024
model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)
```

Embedding ë ˆì´ì–´ëŠ”  ì…ë ¥ í…ì„œì— ë“¤ì–´ìˆëŠ” ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ì›Œë“œ ë²¡í„°ë¡œ ë°”ê¿”ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.

> embedding_sizeëŠ” ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜, ì¦‰ ë‹¨ì–´ê°€ ì¶”ìƒì ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” í¬ê¸°ì´ë‹¤.
ex).
- ì°¨ê°‘ë‹¤:Â [0.0, 1.0]
- ëœ¨ê²ë‹¤:Â [1.0, 0.0]
- ë¯¸ì§€ê·¼í•˜ë‹¤:Â [0.5, 0.5]
> 

### ëª¨ë¸ í™•ì¸

```python
# ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„° í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
# ì§€ê¸ˆì€ ë™ì‘ ì›ë¦¬ì— ë„ˆë¬´ ë¹ ì ¸ë“¤ì§€ ë§ˆì„¸ìš”~
for src_sample, tgt_sample in dataset.take(1): break

# í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ë´…ë‹ˆë‹¤
model(src_sample)
'''
>>>
tf.Tensor: shape=(256, 20, 7001)
'''
```

shape=(batch size, ìì‹ ì—ê²Œ ì…ë ¥ëœ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë§Œí¼ ë™ì¼í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥ ,Dense layerì˜ ì¶œë ¥ ì°¨ì›ìˆ˜)

```python
model.summary()
'''
>>>
Model: "text_generator_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      multiple                  1792256   
_________________________________________________________________
lstm_2 (LSTM)                multiple                  5246976   
_________________________________________________________________
lstm_3 (LSTM)                multiple                  8392704   
_________________________________________________________________
dense_1 (Dense)              multiple                  7176025   
=================================================================
Total params: 22,607,961
Trainable params: 22,607,961
Non-trainable params: 0
_________________________________________________________________
'''
```

output shapeë¥¼ ì•Œë ¤ì£¼ì§€ ì•ŠëŠ”ë‹¤. ê·¸ ì´ìœ ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì´ë‹¤.

### ëª¨ë¸ í•™ìŠµ

```python
# optimizerì™€ lossë“±ì€ ì°¨ì°¨ ë°°ì›ë‹ˆë‹¤
# í˜¹ì‹œ ë¯¸ë¦¬ ì•Œê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”
# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
# https://www.tensorflow.org/api_docs/python/tf/keras/losses
# ì–‘ì´ ìƒë‹¹íˆ ë§ì€ í¸ì´ë‹ˆ ì§€ê¸ˆ ë³´ëŠ” ê²ƒì€ ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True,
    reduction='none'
)

model.compile(loss=loss, optimizer=optimizer)
model.fit(dataset, epochs=30)
'''
>>>
Epoch 30/30
93/93 [==============================] - 18s 193ms/step - loss: 1.3930
'''
```

## 3. ëª¨ë¸ í‰ê°€

ëª¨ë¸ì´ ì‘ë¬¸ì„ ì˜í•˜ëŠ” ì§€ ì»´í“¨í„° ì•Œê³ ë¦¬ì¦˜ì´ í‰ê°€í•˜ëŠ” ê²ƒì€ ë¬´ë¦¬ê°€ ìˆë‹¤. ì‘ë¬¸ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì€ **ì‘ë¬¸ì„ ì‹œì¼œë³´ê³  ì§ì ‘ í‰ê°€** í•˜ëŠ” ê²ƒì´ë‹¤.

```python
def generate_text(model, tokenizer, init_sentence="<start>", max_len=20):
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤
    test_input = tokenizer.texts_to_sequences([init_sentence])
    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)
    end_token = tokenizer.word_index["<end>"]

    # ë‹¨ì–´ í•˜ë‚˜ì”© ì˜ˆì¸¡í•´ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤
    #    1. ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤
    #    2. ì˜ˆì¸¡ëœ ê°’ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ word indexë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤
    #    3. 2ì—ì„œ ì˜ˆì¸¡ëœ word indexë¥¼ ë¬¸ì¥ ë’¤ì— ë¶™ì…ë‹ˆë‹¤
    #    4. ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í–ˆë‹¤ë©´ ë¬¸ì¥ ìƒì„±ì„ ë§ˆì¹©ë‹ˆë‹¤
    while True:
        # 1
        predict = model(test_tensor) 
        # 2
        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] 
        # 3 
        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)
        # 4
        if predict_word.numpy()[0] == end_token: break
        if test_tensor.shape[1] >= max_len: break

    generated = ""
    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ 
    for word_index in test_tensor[0].numpy():
        generated += tokenizer.index_word[word_index] + " "

    return generated
```

```python
generate_text(model, tokenizer, init_sentence="<start> he")
'''
>>>
'<start> he s not fourteen . <end> '
'''
```