# Fundamental 27 Likelihood(MLEì™€ MAP)

# í™•ë¥  ë³€ìˆ˜ë¡œì„œì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°

ê°„ë‹¨í•œ ì˜ˆì‹œì˜ ì¼ì°¨í•¨ìˆ˜ ëª¨ë¸

![Untitled](image/27-.png)

- ìœ„ ì‹ì—ì„œ ì‹¤ìˆ˜ a,bëŠ” fë¼ëŠ” í•¨ìˆ˜ë¡œ í‘œí˜„ë˜ëŠ” ëª¨ë¸ì˜ í˜•íƒœë¥¼ ê²°ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤.

> **íŒŒë¼ë¯¸í„° ê³µê°„(Parameter space)**
(a, b)ê°€ ìœ„ì¹˜í•˜ëŠ” Rê³µê°„
> 

![Untitled](image/27-%202.png)

- ìœ„ ê·¸ë¦¼ì—ì„œ íŒŒë¼ë¯¸í„° ê³µê°„ì— ì£¼ì–´ì§„ í™•ë¥  ë¶„í¬ëŠ” í‰ê· ì´ (1,0)ì¸ ì •ê·œë¶„í¬ì´ë¯€ë¡œ y = ax + bì—ì„œ aì™€bì˜ ê°’ì´ ê°ê° 1ê³¼ 0ì— ê°€ê¹Œìš¸ í™•ë¥ , ê·¸ëŸ¬ë‹ˆê¹Œ ëª¨ë¸ì´ y = xì— ê°€ê¹Œìš¸ í™•ë¥ ì´ í¬ë‹¤ê³  ë³´ëŠ” ê²ƒì´ë‹¤.

# posteriorì™€ prior, likelihood ì‚¬ì´ì˜ ê´€ê³„

- ë² ì´ì§€ì•ˆ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•™ì‹¬ ì•„ì´ë””ì–´ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •ëœ ê°’ì´ ì•„ë‹Œ ë¶ˆí™•ì‹¤ì„±(uncertaionty)ì„ ê°€ì§„ í™•ë¥  ë³€ìˆ˜ë¡œ ë³´ëŠ” ê²ƒ, ë°ì´í„°ë¥¼ ê´€ì°°í•˜ë©´ì„œ ì—…ë°ì´íŠ¸ë˜ëŠ” ê°’ìœ¼ë¡œ ë³´ëŠ” ê²ƒì´ë‹¤.

### ì‚¬ì „ í™•ë¥ , ê°€ëŠ¥ë„, ì‚¬í›„ í™•ë¥ (prior, likelihood, posterior)

- ë°ì´í„°ì˜ ì§‘í•© Xê°€ ì£¼ì–´ì¡Œë‹¤. ê·¸ëŸ¬ë©´ ë°ì´í„°ê°€ ë”°ë¥´ëŠ” ì–´ë–¤ í™•ë¥  ë¶„í¬ p(x)ë„ ìˆì„ ê²ƒì´ë‹¤. ìš°ë¦¬ì˜ ëª©í‘œëŠ” p(x)ë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¸ëŠ” ì¼ì°¨í•¨ìˆ˜ ëª¨ë¸ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.

![Untitled](image/27-%203.png)

ë°ì´í„°ë¥´ ê´€ì°°í•˜ê¸° ì „ íŒŒë¼ë¯¸í„° ê³µê°„ì— ì£¼ì–´ì§„ í™•ë¥  ë¶„í¬ p(ğœ½)ë¥¼ **prior(prior probability, ì‚¬ì „ í™•ë¥ )**ì´ë¼ê³  í•œë‹¤.

ë§Œì•½ priorë¶„í¬ë¥¼ ê³ ì •ì‹œí‚¨ë‹¤ë©´, ì£¼ì–´ì§„ íŒŒë¼ë¯¸í„° ë¶„í¬ì— ëŒ€í•´ì„œ ìš°ë¦¬ê°€ ê°–ê³  ìˆëŠ” ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ â€˜ê·¸ëŸ´ë“¯í•œì§€â€™ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ ë‚˜íƒ€ë‚¸ëŠ” ê°’ì´ **likelihood(ê°€ëŠ¥ë„, ìš°ë„)ì´ë‹¤.**

![Untitled](image/27-%204.png)

ì¦‰, íŒŒë¼ë¯¸í„°ì˜ ë¶„í¬ p(ğœ½)ê°€ ì •í•´ì¡Œì„ ë•Œ xë¼ëŠ” ë°ì´í„°ê°€ ê´€ì°°ë  í™•ë¥ ì´ë‹¤.

Likeihoodê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ê³· ìš°ë¦¬ê°€ ì§€ì •í•œ íŒŒë¼ë¯¸í„° ì¡°ê±´ì—ì„œ ë°ì´í„°ê°€ ê´€ì°°ë  í™•ë¥ ì´ ë†’ë‹¤ëŠ” ê²ƒì´ê³ , ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ëª¨ë¸ì´ ì˜ í‘œí˜„í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í• ìˆ˜ ìˆë‹¤.

ì´ë ‡ê²Œ ë°ì´í„°ë“¤ì˜ likelihoodê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ì •(maximum likelihood estimation, MLE)ì´ë¼ê³  í•œë‹¤.

ë°˜ëŒ€ë¡œ, ë°ì´í„° ì§‘í•© Xê°€ ì£¼ì–´ì¡Œì„ ë•Œ íŒŒë¼ë¯¸í„° ğœ½ì˜ ë¶„í¬ p(ğœ½|X)ë¥¼ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ë•Œ ì´ ê°’ì„ â€˜ë°ì´í„°ë¥¼ ê´€ì°°í•œ í›„ ê³„ì‚°ë˜ëŠ” í™•ë¥ â€™ì´ë¼ëŠ” ëœ»ì—ì„œ posterior(posterior probability, ì‚¬í›„ í™•ë¥ )ì´ë¼ê³  ë¶€ë¥¸ë‹¤.

ì‹¤ì œë¡œëŠ” ë°ì´í„° í¬ì¸íŠ¸ì˜ ê°œìˆ˜ëŠ” ìœ í•œí•˜ê¸° ë•Œë¬¸ë° ë°ì´í„°ê°€ ë”°ë¥´ëŠ” í™•ë¥  ë¶„í¬ p(x)ëŠ” ìš°ë¦¬ê°€ ì •í™•í•˜ê²Œ ì•Œ ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª©í‘œê°€ p(x)ë¥¼ ì§ì ‘ êµ¬í•  ìˆ˜ê°€ ì—†ìœ¼ë‹ˆê¹Œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ğœ½ë¥¼ ì¡°ì ˆí•´ê°€ë©´ì„œ ê°„ì ‘ì ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì´ë‹¤.

ë”°ë¼ì„œ posteriorë¥¼ ì§ì ‘ ê³„ì‚°í•´ì„œ ìµœì ì˜ ğœ½ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ priorì™€ likelihoodì— ê´€í•œ ì‹ìœ¼ë¡œ ë³€í˜•í•œ ë‹¤ìŒ, ê·¸ ì‹ì„ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ğœ½ë¥¼ ì°¾ëŠ”ë‹¤.

ì´ë ‡ê²Œ posteriorë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ìµœëŒ€ ì‚¬í›„ í™•ë¥  ì¶”ì •(maximum aposteriori estimation, MAP)ë¼ê³  í•œë‹¤.

### posteriorì™€ prior, likelihood ì‚¬ì´ì˜ ê´€ê³„

![Untitled](image/27-%205.png)

![Untitled](image/27-%206.png)

- ì •í™•í•œ p(x)ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— posterior p(ğœ½|X)ì˜ ê°’ë„ ì§ì ‘ êµ¬í•  ìˆ˜ ì—†ë‹¤. í•˜ì§€ë§Œ p(x)ëŠ” ê³ ì •ëœ ê°’ì´ê³  likehoodì™€ priorëŠ” ê³„ì‚°ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ìš°ë³€ì„ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì€ êµ¬í•  ìˆ˜ ìˆë‹¤.

# likelihoodì™€ ë¨¸ì‹ ëŸ¬ë‹

- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬ë¥¼ ê·¼ì‚¬í•œëŠ” ì—­í• ì„ í•˜ê¸° ë•Œë¬¸ì—, ë°ì´í„°ê°€ ë“¤ì–´ì™€ë„ 100%ì˜ ì •í™•ë„ë¥¼ ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê¸°ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤.
- ë”°ë¼ì„œ ë°ì´í„°ë¡œë¶€í„° ì˜ˆì¸¡í•œ predictionê³¼ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ë°ì´í„°ì˜ label ì‚¬ì´ì—ëŠ” ì˜¤ì°¨ê°€ ìƒê¸°ê²ŒëœëŠ”ë°, ìš°ë¦¬ì—ê²Œ ê´€ì°°ë˜ëŠ” ë°ì´í„°ì—ëŠ” ì´ë¯¸ ë…¸ì´ì¦ˆê°€ ì„ì—¬ìˆì–´ì„œ ì´ëŸ° ì˜¤ì°¨ê°€ ë°œìƒí•œë‹¤ê³  í•´ì„í•œë‹¤.

![Untitled](image/27-%207.png)

ì§€ë„ í•™ìŠµì˜ ì˜ˆì‹œì—ì„œëŠ”, íŒŒë¼ë¯¸í„° ë¶„í¬ ğœ½ì™€ ì…ë ¥ ë°ì´í„° $x_n$ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë¼ë²¨ $y_n$ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œê°€ ëœë‹¤. ì…ë ¥ ë°ì´í„°ì˜ ì§‘í•©ì„ X, ë¼ë²¨ë“¤ì˜ ì§‘í•©ì„ Yë¼ê³  í• ë•Œ, likelihoodëŠ” íŒŒë¼ë¯¸ë„ˆì™€ ì…ë ¥ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì¶œë ¥ê°’(ë¼ë²¨)ì˜ í™•ë¥  ë¶„í¬, ì¦‰ p(Y|ğœ½, X)ê°€ ëœë‹¤.

![Untitled](image/27-%208.png)

# likelihood ê° ì¡ê¸°

ëœë¤í•œ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì¢Œí‘œí‰ë©´ ìœ„ì— í‘œì‹œí•´ ì£¼ëŠ” ì½”ë“œ

```python
import math
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(321)

input_data = np.linspace(-2, 2, 5)
label = input_data + 1 + np.random.normal(0, 1, size=5)

plt.scatter(input_data, label)
plt.show()
```

![Untitled](image/27-%209.png)

```python
# model: y = ax + b
# a, b ê°’ì„ ë°”ê¾¸ë©´ì„œ ì‹¤í–‰í•´ë³´ì„¸ìš”
#-------------------------------#
a = 1
b = 1
#-------------------------------#

# ëª¨ë¸ ì˜ˆì¸¡ê°’
model_output = a*input_data + b
likelihood = []

# x: ì…ë ¥ë°ì´í„°, y: ë°ì´í„°ë¼ë²¨
# ì˜ˆì¸¡ê°’ê³¼ ë¼ë²¨ì˜ ì°¨ì´ë¥¼ ì œê³±í•´ expì— ì‚¬ìš©
for x, y, output in zip(input_data, label, model_output):
    likelihood.append(1/(math.sqrt(2*math.pi*0.1*0.1))*math.exp(-pow(y-output,2)/(2*0.1*0.1)))

model_x = np.linspace(-2, 2, 50)
model_y = a*model_x + b

fig, ax = plt.subplots()
ax.scatter(input_data, label)
ax.plot(model_x, model_y)

for i, text in enumerate(likelihood):
    ax.annotate('%.3e'%text, (input_data[i], label[i]))

plt.show()
```

![Untitled](image/27-%2010.png)

### likelihoodê°€ ì™œ ì¤‘ìš”í•œê°€?

- ìœ„ì— ì˜ˆì œì—ì„œ ë³´ì´ë“¯, ë°ì´í„° í¬ì¸íŠ¸ê°€ ëª¨ë¸ í•¨ìˆ˜ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ë°ì´í„°ì˜ liokelihoodëŠ” ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œí•œë‹¤.

# MLE: ìµœëŒ€ ê°€ëŠ¥ë„ ì¶”ë¡ 

### ë°ì´í„°ì…‹ ì „ì²´ì˜ likelihood

ëª¨ë¸ íŒŒë¼ë¯¸í„° ğœ½ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë°ì´í„° í¬ì¸íŠ¸$(x_n, y_n)$ì˜ likelihoodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/27-%2011.png)

ì „ì œ ë°ì´í„°ì…‹ì˜ ëŒ€í•˜ì—¬ likelihoodêµ¬í•˜ê¸°

- ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„° í¬ì¸íŠ¸$(x_1, y_1),...,(x_n, y_n)$ì€ ì„œë¡œ ë…ë¦½ì´ê³ (independent) ê°™ì€ í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³ (identicallu distributed) ê°€ì •í•œë‹¤.
- ì´ ì¡°ê±´ì„ ì¤„ì—¬ì„œ i.i.dë¼ê³  ë¶€ë¥´ëŠ” ë°, ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œì—ì„œê¼­ í•„ìš”í•œ ì „ì œì¡°ê±´ì´ë‹¤.
- ë°ì´í„° í¬ì¸íŠ¸ë“¤ì´ ì„œë¡œ ë…ë¦½ì´ë¯€ë¡œ, ë°ì´í„°ì…‹ ì „ì²´ì˜ likelihood p(Y|ğœ½, X)ëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°ê°ì˜ likelihoodë¥¼ ëª¨ë‘ ê³±í•œ ê°’ê³¼ ê°™ë‹¤.

![Untitled](image/27-%2012.png)

ë¡œê·¸ë¥¼ ì”Œìš°ë©´ ê³±ì…ˆ ì—°ì‚°ì´ ë§ì…ˆ ì—°ì‚°ìœ¼ë¡œ ë°”ë€Œë©´ì„œ ë¯¸ë¶„ ê³„ì‚°ì´ í¸ë¦¬í•´ ì§€ê¸°ë•Œë¬¸ì— MLEë¥¼ ì‹¤ì œë¡œ ì ìš©í•  ë•ŒëŠ” likelihood ëŒ€ì‹  log likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•œë‹¤.

ë˜í•œ ë¡œê·¸ í•¨ìˆ˜ëŠ” ë‹¨ì¡° ì¦ê°€(monotonically increasing)í•˜ë¯€ë¡œ likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ì™€ log likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì´ ê°™ì•„ì„œ í•™ìŠµ ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤.

![Untitled](image/27-%2013.png)

![Untitled](image/27-%2014.png)

ë˜í•œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê´€ì €ì—ì„œ, log likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ëŒ€ì‹  negative log likelihood (-log p(Y|ğœ½. X))ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê¸°ë„ í•œë‹¤.

![Untitled](image/27-%2015.png)

![Untitled](image/27-%2016.png)

ìœ ë„ê³¼ì •

![Untitled](image/27-%2017.png)

![Untitled](image/27-%2018.png)

![Untitled](image/27-%2019.png)

# MLE ìµœì í•´ êµ¬í•˜ê¸°

### ë°ì´í„°ì…‹ ìƒì„±

- y = x + 1
- ë°ì´í„° í¬ì¸íŠ¸ 20ê°œ
- í‰ê·  0, í‘œì¤€í¸ì°¨ 0.5

```python
import math
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
num_samples = 20

input_data = np.linspace(-2, 2, num_samples)
labels = input_data + 1 + np.random.normal(0, 0.5, size=num_samples)

plt.scatter(input_data, labels)
plt.show()
```

![Untitled](image/27-%2020.png)

![Untitled](image/27-%2021.png)

```python
def likelihood(labels, preds):
    result = 1/(np.sqrt(2*math.pi*0.1*0.1))*np.exp(-np.power(labels-preds,2)/(2*0.1*0.1))
    
    return np.prod(result)

def neg_log_likelihood(labels, preds):
    const_term = len(labels)*math.log(1/math.sqrt(2*math.pi*0.1*0.1))

    return (-1)*(const_term + 1/(2*0.1*0.1)*np.sum(-np.power(labels-preds,2)))
```

```python
# X: 20x2 matrix, y: 20x1 matrix
# input_data ë¦¬ìŠ¤íŠ¸ë¥¼ column vectorë¡œ ë°”ê¾¼ ë‹¤ìŒ np.append í•¨ìˆ˜ë¡œ ìƒìˆ˜í•­ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
X = np.append(input_data.reshape((-1, 1)), np.ones((num_samples, 1)), axis=1)
y = labels

theta_1, theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)

print('slope: '+'%.4f'%theta_1+' bias: '+'%.4f'%theta_0)

predictions = theta_1 * input_data + theta_0
print('likelihood: '+'%.4e'%likelihood(labels, predictions))
print('negative log likelihood: '+'%.4e'%neg_log_likelihood(labels, predictions))

model_x = np.linspace(-2, 2, 50)
model_y = theta_1 * model_x + theta_0

plt.scatter(input_data, labels)
plt.plot(model_x, model_y)
plt.show()
```

![Untitled](image/27-%2022.png)

# MAP: ìµœëŒ€ ì‚¬í›„ í™•ë¥  ì¶”ì •

MLEë¡œ êµ¬í•œ ì²˜ì ì˜ íŒŒë¼ë¯¸í„° ì‹: $**\mathbf{\theta}_{ML} = (X^\top X)^{-1}X^\top\mathbf{y}X**$

MLEì˜ ìµœì í•´ëŠ” ì˜¤ë¡œì§€ ê´€ì¸¡ëœ ë°ì´í„° ê°’ì—ë§Œ ì˜ì¡´í•œë‹¤. ë”°ë¼ì„œ ê´€ì¸¡ëœ ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ë§ì´ ì„ì—¬ ìˆëŠ” ê²½ìš°, ì´ìƒì¹˜ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ëŠ” ëª¨ë¸ì˜ ì•ˆì •ì„±ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

MAPëŠ” ë°ì´í„°ê°€ ìˆì„ ë•Œ íŒŒë¼ë¯¸í„°ì˜ ê°’ì´ ë¬´ì—‡ì¼ í™•ë¥ ì´ ì œì¼ ë†’ì€ê°€?ì˜ ë¬¸ì œì´ë‹¤.

ì§€ë„ í•™ìŠµì˜ ê²½ìš° posteriorëŠ” p(ğœ½|X, Y)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ”ë° ì´ ì‹ì„ prior p(ğœ½)ì™€ likelihood p(Y|ğœ½, X)ì— ê´€í•œ ì‹ìœ¼ë¡œ ë³€í˜•í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/27-%2023.png)

![Untitled](image/27-%2024.png)

MLEì—ì„œ negative log likelihoodë¥¼ ìµœì†Œí™”í–ˆë˜ ê²ƒê³¼ ê°™ì´, MAPì—ì„œë„ ì‹¤ì œë¡œëŠ” posteriorë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ëŒ€ì‹  negative log posteriorë¥¼ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì„ êµ¬í•œë‹¤. ì‹ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/27-%2025.png)

ë˜í•œ log p(Y|X)ë¶€ë¶„ì€ ğœ½ì— ì˜í•œ ì‹ì´ ì•„ë‹ˆë¯€ë¡œ ì œì™¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/27-%2026.png)

![Untitled](image/27-%2027.png)

MLEì—ì„œ í–ˆë˜ê²ƒì²˜ëŸ¼ MAPì—ì„œë„ ë¯¸ë¶„ì„ í•˜ì—¬ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.

![Untitled](image/27-%2028.png)

![Untitled](image/27-%2029.png)

ë”°ë¼ì„œ MAPì˜ ìµœì  íŒŒë¼ë¯¸í„° ğœ½ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/27-%2030.png)

![Untitled](image/27-%2031.png)

# MLEì™€ MAPì˜ ë¹„êµ

### ë°ì´í„°ì…‹

```python
import math
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
num_samples = 10

input_data = np.linspace(-2, 2, num_samples)
labels = input_data + 1 + np.random.normal(0, 0.5, size=num_samples)

input_data = np.append(input_data, [0.5, 1.5])
labels = np.append(labels, [9.0, 10.0])

plt.scatter(input_data, labels)
plt.show()
```

![Untitled](image/27-%2032.png)

### íŒŒë¼ë¯¸í„° ê³„ì‚°

```python
def likelihood(labels, preds):
    result = 1/(np.sqrt(2*math.pi*0.1*0.1))*np.exp(-np.power(labels-preds,2)/(2*0.1*0.1))
    
    return np.prod(result)

def neg_log_likelihood(labels, preds):
    const_term = len(labels)*math.log(1/math.sqrt(2*math.pi*0.1*0.1))

    return (-1)*(const_term + 1/(2*0.1*0.1)*np.sum(-np.power(labels-preds,2)))
```

```python
# X: 21x2 matrix, y: 21x1 matrix
# input_data ë¦¬ìŠ¤íŠ¸ë¥¼ column vectorë¡œ ë°”ê¾¼ ë‹¤ìŒ np.append í•¨ìˆ˜ë¡œ ìƒìˆ˜í•­ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
X = np.append(input_data.reshape((-1, 1)), np.ones((num_samples+2, 1)), axis=1)
y = labels

# MLE íŒŒë¼ë¯¸í„° ê³„ì‚°ì‹
mle_theta_1, mle_theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)
# MAP íŒŒë¼ë¯¸í„° ê³„ì‚°ì‹
map_theta_1, map_theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)+(0.1*0.1)/(0.04*0.04)*np.eye(2)), X.T), y)

print('[MLE result] (blue)')
print('slope: '+'%.4f'%mle_theta_1+' bias: '+'%.4f'%mle_theta_0)

mle_preds = mle_theta_1 * input_data + mle_theta_0
print('likelihood: '+'%.4e'%likelihood(labels, mle_preds))
print('negative log likelihood: '+'%.4e\n'%neg_log_likelihood(labels, mle_preds))

print('[MAP result] (orange)')
print('slope: '+'%.4f'%map_theta_1+' bias: '+'%.4f'%map_theta_0)

map_preds = map_theta_1 * input_data + map_theta_0
print('likelihood: '+'%.4e'%likelihood(labels, map_preds))
print('negative log likelihood: '+'%.4e'%neg_log_likelihood(labels, map_preds))

model_x = np.linspace(-2, 2, 50)
mle_model_y = mle_theta_1 * model_x + mle_theta_0
map_model_y = map_theta_1 * model_x + map_theta_0

plt.scatter(input_data, labels)
plt.plot(model_x, mle_model_y)
plt.plot(model_x, map_model_y)
plt.show()
'''
[MLE result] (blue)
slope: 1.4748 bias: 2.4784
likelihood: 0.0000e+00
negative log likelihood: 4.1298e+03

[MAP result] (orange)
slope: 1.1719 bias: 1.6628
likelihood: 0.0000e+00
negative log likelihood: 4.6645e+03
'''
```

![Untitled](image/27-%2033.png)