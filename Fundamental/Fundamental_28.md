# Fundamental 28 ì •ë³´ì´ë¡  í†ºì•„ë³´ê¸°

# Information Content

> **ì •ë³´ ì´ë¡ (information theory)ì´ë€?**
ì¶”ìƒì ì¸ ì •ë³´ë¼ëŠ” ê°œë…ì„ ì •ëŸ‰í™”í•˜ê³  ì •ë³´ì˜ ì €ì¥ê³¼ í†µì‹ ì„ ì—°êµ¬í•˜ëŠ” ë¶„ì•¼ì´ë‹¤.
> 

![Untitled](image/28-.png)

- ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ì£¼ë¨¸ë‹ˆì—ì„œ ê³µì„ í•˜ë‚˜ì”© ë½‘ì•„ ë‚˜ì—´í•œë‹¤ê³  ê°€ì •í• ë•Œ ì˜¤ë¥¸ìª½ë³´ë‹¤ ì™¼ìª½ì´ ë” ì •ë³´ëŸ‰ì´ ë§ë‹¤.

ê·¸ë ‡ë‹¤ë©´ ë§Œì•½ íŒŒë€ìƒ‰ê³µ999ê°œì™€ ë¹¨ê°„ìƒ‰ê³µ 1ê°œê°€ ë“¤ì–´ìˆëŠ” ì£¼ë¨¸ë‹ˆê°€ ìˆê³  ê³µì„ í•˜ë‚˜ê±°ë‚´ê³  ë‹¤ì‹œ ë„£ëŠ” ì‹¤í—˜ì„ ë°˜ë³µí• ë•Œ ì²˜ìŒì˜ íŒŒë€ê³µì€ ì •ë³´ëŸ‰ì´ ë†’ì§€ë§Œ ìˆ˜ì‹­, ìˆ˜ë°±ë²ˆì˜ ë°˜ë³µë’¤ì— íŒŒë€ê³µì€ í™•ë¥ ì´ 1ì— ê°€ê¹ê¸° ë•Œë¬¸ì— ì •ë³´ëŸ‰ì´ ë§¤ìš° ë‚®ë‹¤. ë°˜ë©´ì— ë¹¨ê°„ê³µì€ ì •ë³´ëŸ‰ì´ ë§¤ìš° ë†’ì„ ê²ƒì´ë‹¤.

ì •ë³´ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì„¸ ê°€ì§€ ì¡°ê±´

- ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚¬ê±´ì€ ì •ë³´ëŸ‰ì´ ë‚®ê³ , ë°˜ë“œì‹œ ì¼ì–´ë‚˜ëŠ” ì‚¬ê±´ì—ëŠ” ì •ë³´ê°€ ì—†ëŠ” ê²ƒì´ë‚˜ ë§ˆì°¬ê°€ì§€ì´ë‹¤.
- ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì´ ë‚®ì€ ì‚¬ê±´ì€ ì •ë³´ëŸ‰ì´ ë†’ë‹¤.
- ë‘ ê°œì˜ ë…ë¦½ì ì¸ ì‚¬ê±´ì´ ìˆì„ ë•Œ, ì „ì²´ ì •ë³´ëŸ‰ì€ ê°ê°ì˜ ì •ë³´ëŸ‰ì„ ë”í•œ ê²ƒê³¼ ê°™ë‹¤.

ì‚¬ê±´ xê°€ ì¼ì–´ë‚  í™•ë¥ ì„ P(X=x)ë¼ê³  í•  ë•Œ, ì‚¬ê±´ì˜ ì •ë³´ëŸ‰(information content) I(x)ëŠ” ë‹¤ìŒê³¼ê°™ì´ ì •ì˜ëœë‹¤.

![Untitled](image/28-%201.png)

![Untitled](image/28-%202.png)

íŒŒë€ê³µ nê°œì™€ ë¹¨ê°„ ê³µ 1ê°œê°€ ìˆì„ ë•Œ ë¹¨ê°„ìƒ‰ ê³µì„ ë½‘ëŠ” ì‚¬ê±´ì˜ ì •ë³´ëŸ‰

```python
import numpy as np
import math
import random

# ì£¼ë¨¸ë‹ˆ ì†ì— ë“¤ì–´ìˆëŠ” ê³µì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. ìˆ«ìë¥¼ ë°”ê¾¸ë©´ì„œ ì‹¤í—˜í•´ë³´ì„¸ìš”!
total = 1000

#---------------#

count = 1   # ì‹¤í—˜ì´ ëë‚  ë•Œê¹Œì§€ êº¼ë‚¸ ê³µì˜ ê°œìˆ˜

# 1ë¶€í„° totalê¹Œì§€ì˜ ì •ìˆ˜ ì¤‘ì—ì„œ í•˜ë‚˜ë¥¼ ë½‘ê³  totalê³¼ ê°™ìœ¼ë©´ ì‹¤í—˜ ì¢…ë£Œ
# total=1000ì¸ ê²½ìš° 1~999: blue / 1000: red
while True:
    sample = random.randrange(1,total+1)
    if sample == total:
        break
    count += 1

print('number of blue samples: '+str(count-1))
print('information content: '+str(-math.log(1/count)))
'''
number of blue samples: 595
information content: 6.39024066706535
'''
```

# Entropy

- ì •ë³´ëŸ‰ì€ í•œ ê°€ì§€ ì‚¬ê±´ì— ëŒ€í•œ ê°’ì´ë‹¤.
- íŠ¹ì • í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ì‚¬ê±´ë“¤ì˜ ì •ë³´ëŸ‰ ê¸°ëŒ“ê°’ì„ **ì—”íŠ¸ë¡œí”¼(entropy)**ë¼ê³  í•œë‹¤.

### For Discrete Random Variables

- ì´ì‚° í™•ë¥  ë³€ìˆ˜ Xê°€ $x_1,x_2,...,x_n$ì¤‘ í•˜ë‚˜ì˜ ê°’ì„ ê°€ì§„ë‹¤ê³  ê°€ì •í•  ë•Œ **ì—”íŠ¸ë¡œí”¼ëŠ” ê°ê°ì˜ ê²½ìš°ì˜ ìˆ˜ê°€ ê°€ì§€ëŠ” ì •ë³´ëŸ‰ì— í™•ë¥ ì„ ê³±í•œ í›„, ê·¸ ê°’ì„ ëª¨ë‘ ë”í•œ ê°’**ì´ë‹¤.

![Untitled](image/28-%203.png)

- ì—¬ëŸ¬ ê°€ì§€ ìƒ‰ê¹¡ì˜ ê³µì´ ë“¤ì–´ìˆì„ë•Œ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ê³ , ê°™ì€ ìƒ‰ê¹”ì˜ ê³µì´ ë§ì´ ë“¤ì–´ìˆì„ ë•Œ ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ë‹¤.
- ì´ëŸ¬í•œ ì—”íŠ¸ë¡œí”¼ì˜ ì§ê´€ì ì¸ ê°œë…ì€ ë¬´ì§ˆì„œë„ ë˜ëŠ” ë¶ˆí™•ì‹¤ì„±ê³¼ë„ ë¹„ìŠ·í•˜ë‹¤.

![Untitled](image/28-%204.png)

- ì‚¬ê±´ë“¤ì˜ í™•ë¥ ì´ ê· ë“±í• ìˆ˜ë¡ ì—”íŠ¸ë¡œí”¼ ê°’ì€ ì¦ê°€í•œë‹¤.

ë™ì „ì„ ë˜ì§ˆ ë•Œ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì— ë”°ë¥¸ ì—”íŠ¸ë¡œí”¼ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ (ê· ë“±ë¶„í¬)

![Untitled](image/28-%205.png)

- ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ 90%ì¸ ë™ì „ì„ ë˜ì§ˆ ë•Œë³´ë‹¤ ì•ë©´ê³¼ ë’·ë©´ì˜ í™•ë¥ ì´ ê°™ì€ ë™ì „ì„ ë˜ì§ˆ ë•Œ ê²°ê³¼ë¥¼ ì—ì¸¡í•˜ê¸°ê°€ ë” ì–´ë µê¸° ë•Œë¬¸ì— ë¶ˆí™•ì‹¤ì„±ì´ í¬ë‹¤ê³  í• ìˆ˜ ìˆë‹¤.

### For Contiuous Random Variables

- Xê°€ ì´ì‚° í™•ë¥  ë³€í›„ ì¼ë•Œ ì—”íŠ¸ë¡œí”¼ëŠ” ìœ„ì™€ ê°™ì´ ì •ë³´ëŸ‰ì— í™•ë¥ ì„ ê°ê° ê³±í•´ì„œ ëª¨ë‘ ë”í•œ ê°’ìœ¼ë¡œ ì •ì˜ëœë‹¤.
- Xê°€ ì—°ì†ì ì¸ ê°’ì„ ê°–ëŠ” ì—°ì† í™•ë¥  ë³€ìˆ˜ ì¼ë•ŒëŠ” ìœ í•œí•© ëŒ€ì‹  ì ë¶„ì˜ í˜•íƒœë¡œ ì •ì˜ í•œë‹¤.

í™•ë¥  ë³€ìˆ˜ Xì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ê°€ p(x)ì¼ ë•Œ ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/28-%206.png)

- ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ì´ì‚° í™•ë¥  ë³€ìˆ˜ì™€ êµ¬ë¶„í•˜ì—¬ **ë¯¸ë¶„ ì—”íŠ¸ë¡œí”¼**í•˜ê³  ë¶€ë¥¸ë‹¤.

# Kullback Leibler Divergence

ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª©í‘œëŠ” ìƒˆë¡œìš´ ì…ë ¥ ë°ì´í„°ê°€ ë“¤ì–´ì™€ë„ ì˜ˆì¸¡ì´ ì˜ ë˜ë„ë¡, ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë°ì´í„°ì˜ ì‹¤ì œ í™•ë¥  ë¶„í¬ì— ê°€ê¹ê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤.

ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª¨ë¸ì€ í¬ê²Œ ë‘ ê°€ì§€ê°€ ìˆë‹¤. 

- ê²°ì • ëª¨ë¸(discriminative model): ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬ë¥¼ ëª¨ë¸ë§ í•˜ì§€ ì•Šê³  ê²°ì • ê²½ê³„(dicision boundary)ë§Œì„ í•™ìŠµí•œë‹¤. ex) ëª¨ë¸ì˜ ê²°ê³¼ê°’ì´ 0ë³´ë‹¤ ì‘ì„ ê²½ìš° ë°ì´í„°ë¥¼ 1ë²ˆ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ê³ , 0ë³´ë‹¤ í´ ê²½ìš° 2ë²ˆ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•œë‹¤.
- ìƒì„± ëª¨ë¸(generative model): ë°ì´í„°ì™€ ëª¨ë¸ë¡œë¶€í„° ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ í™•ë¥  ë¶„í¬ì™€ ë² ì´ì¦ˆ ì´ë¡ ì„ ì´ìš©í•´ì„œ ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•œë‹¤.

ìƒì„± ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•ŒëŠ” ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚¸ëŠ” ì§€í‘œê°€ í•„ìš”í•˜ë‹¤. ì§€í‘œì¤‘ ëŒ€í‘œì ì¸ ì˜ˆê°€ **ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(Kullback-Leibler divergence, KL divergence)**ì´ë‹¤.

### KL divergence

ë°ì´í„°ê°€ ë‹¤ë¥´ëŠ” ì‹¤ì œ í™•ë¥  ë¶„í¬ë¥¼ P(x), ëª¨ë¸ì´ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥  ë¶„í¬ë¥¼ Q(x)ë¼ê³  í•  ë•Œ, ë‘ í™•ë¥  ë¶„í¬ì˜ KL divergenceëŠ” P(x)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ëœ Q(x)ì˜ í‰ê·  ì •ë³´ëŸ‰ê³¼, P(x)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ëœ P(x)ì˜ í‰ê·  ì •ë³´ëŸ‰ì˜ ì°¨ì´ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

ì´ì‚° í™•ë¥  ë³€ìˆ˜

![Untitled](image/28-%207.png)

ì—°ì† í™•ë¥  ë³€ìˆ˜

![Untitled](image/28-%208.png)

íŠ¹ì§• 

- $D_{KL}(P||Q)$ â‰¥ 0
- $D_{KL}(P||Q) = 0$ if and only if P = Q
- non-symmetric: $D_{KL}(P||Q)$ â‰  $D_{KL}(Q||P)$

ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œì—ì„œëŠ” ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì¤„ì—¬ë´ í•˜ë¯€ë¡œ $D_{KL}(P||Q)$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.

![Untitled](image/28-%209.png)

- ì—¬ê¸°ì„œ P(x)ëŠ” ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬ì´ë¯€ë¡œ ìš°ë¦¬ê°€ ë°”ê¿€ ìˆ˜ ì—†ëŠ” ê³ ì •ëœ ê°’ì´ë‹¤. ë”°ë¼ì„œ KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œëŠ” ê³§ ë¹¨ê°„ìƒ‰ ë¶€ë¶„ì„ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œì´ë‹¤.
- ì´ ë¶€ë¶„ì€ P(x)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•œ Q(x)ì˜ ì—”íŠ¸ë¡œí”¼, ì¦‰ P(x)ì— ëŒ€í•œ Q(x)ì˜ **êµì°¨ ì—”íŠ¸ë¡œí”¼(Cross Entropy)**ì´ë‹¤.

### Cross Entropy

ì—”íŠ¸ë¡œí”¼, êµì°¨ ì—”íŠ¸ë¡œí”¼, KL divergenceì‚¬ì´ì˜ ê´€ê³„ì‹

![Untitled](image/28-%2010.png)

# Cross Entropy Loss

> **ì†ì‹¤ í•¨ìˆ˜(loss function)ì´ë€?**
ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ëª¨ë¸ì´ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥  ë¶„í¬ì™€ ë°ì´í„°ê°€ ë”°ë¥´ëŠ” ì‹¤ì œ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ì†ì‹¤ í•¨ìˆ˜ë¼ê³  í•œë‹¤.
> 

ë°ì´í„°ê°€ ì—°ì†ì ì¸ ê°’ì„ ê°€ì§€ëŠ” íšŒê·€(regression)ë¬¸ì œì™€ëŠ” ë‹¤ë¥´ê²Œ, ì´ì‚°ì ì¸ ê°’ì„ ê°€ì§€ëŠ” ë¶„ë¥˜(classification)ë¬¸ì œì—ì„œëŠ” ëª¨ë¸ì˜ ì¶œë ¥ ê²°ê³¼ê°€ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜(logistic function)ë¡œ í‘œí˜„ëœë‹¤. ë¶„ë¥˜ í´ë˜ìŠ¤ê°€ 2ê°œì¸ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ê°€ nê°œì¼ ë•Œë¡œ í™•ì¥í•œ ê²ƒì´ ë”¥ëŸ¬ë‹ì—ì„œë„ ìì£¼ ì‚¬ìš©ëœëŠ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜(softmax function)ì´ë‹¤.

### example

ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë°ì´í„°ì˜ ë¼ë²¨ì€ one-hot encodingì„ í†µí•´ í‘œí˜„ëœë‹¤.

3ê°œì˜ í´ë˜ìŠ¤ $c_1, c_2, c_3$ê°€ ì¡´ì¬í•˜ëŠ” ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì–´ë–¤ ë°ì´í„°ì˜ ì¶œë ¥ã„±ë°§ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•˜ì.

![Untitled](image/28-%2011.png)

ì´ ê²°ê³¼ëŠ” ê³§ ë‹¤ìŒ ì‹ì„ ë‚˜íƒ€ë‚¸ë‹¤.

![Untitled](image/28-%2012.png)

ë˜í•œ ì •ë‹µë¼ë²¨ì˜ í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ê°™ë‹¤.

![Untitled](image/28-%2013.png)

ì´ê²ƒë“¤ì„ cross entropyë¡œ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/28-%2014.png)

### code

```python
import numpy as np
import random

# generate random output
#-----------------#
# can be modified
class_num = 4
#-----------------#
q_vector = []
total = 1

for i in range(class_num-1):
    q = random.uniform(0,total)
    q_vector.append(round(q,3))
    total = total - q

q_vector.append(total)
softmax_output = np.array(q_vector)

print(softmax_output)
'''
[0.801      0.14       0.055      0.00317765]
'''
```

```python
#-----------------#
# can be modified
class_index = 1
#-----------------#

p_vector = np.zeros(class_num)
p_vector[class_index-1] = 1

cross_entropy = -np.sum(np.multiply(p_vector, np.log(softmax_output)))

print('model prediction: '+str(softmax_output))
print('data label: '+str(p_vector))
print('cross entropy: '+str(round(cross_entropy,4)))
'''
model prediction: [0.801      0.14       0.055      0.00317765]
data label: [1. 0. 0. 0.]
cross entropy: 0.2219
'''
```

### Cross Entropyì™€ Likelihoodì˜ ê´€ê³„

ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ğœ½ë¡œ ë†“ìœ¼ë©´, ëª¨ë¸ì´ í‘œí˜„í•˜ëŠ” í™•ë¥  ë¶„í¬ëŠ” $Q(y|X,\theta)$ë¡œ, ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬ëŠ” $P(y|X)$ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° $Q(y|X,\theta)$ëŠ” ë°ì´í„°ì…‹ê³¼ íŒŒë¼ë¯¸í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì˜ˆì¸¡ê°’ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ë¯€ë¡œ ëª¨ë¸ì˜ likelihoodì™€ ê°™ë‹¤.

![Untitled](image/28-%2015.png)

- ë”°ë¼ì„œ cross entropyë¥¼ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì€ ê²°êµ­ negative log likelihoodë¥¼ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.

# Decision Treeì™€ Entropy

ì—”íŠ¸ë¡œí”¼ ê°œë…ì´ í™œë°œí•˜ê²Œ ì“°ì´ëŠ” ë¶„ì•¼ë¥¼ í•˜ë‚˜ë§Œ ë” ì§šì–´ ë³´ë©´ ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)ê³„ì—´ì˜ ëª¨ë¸ì´ë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ ë‚ ì”¨ì— ë”°ë¥¸ ìš´ë™ê²½ê¸° ì—¬ë¶€ë¥¼ ê¸°ë¡í•œ ë°ì´í„°ì…ë‹ˆë‹¤. Dayë¥¼ ì œì™¸í•˜ê³ , ë‹¤ìŒê³¼ ê°™ì€ 4ê°œì˜ ë°ì´í„° ì»¬ëŸ¼ì´ ìˆìœ¼ë©°, Play(Yes/No) ì»¬ëŸ¼ì´ ë¼ë²¨ ì—­í• ì„ í•˜ê²Œ ëœë‹¤.

- Outlook : ì „ë°˜ì  ë‚ ì”¨ (Sunny(ë§‘ì€), Overcast(êµ¬ë¦„ ë‚€), Rainy(ë¹„ ì˜¤ëŠ”))
- Temperature : ê¸°ì˜¨ ì •ë³´(ì„­ì”¨ì˜¨ë„)
- Humidity : ìŠµë„ ì •ë³´ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜(%), ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜ëœ ê²½ìš° (high, normal))
- Wind : í’ëŸ‰ ì •ë³´ (TRUE(ë°”ëŒ ë¶Š), FALSE(ë°”ëŒ ì•ˆ ë¶Š) )

![Untitled](image/28-%2016.png)

ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ëŠ” ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ì—ì„œ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ë¥¼ ë‚˜ëˆ´ì„ ë•Œ ë‚˜ëˆ„ê¸° ì „ë³´ë‹¤ ì—”íŠ¸ë¡œí”¼ê°€ ê°ì†Œí•˜ëŠ”ì§€ë¥¼ ë”°ì ¸ì„œ, ì—”íŠ¸ë¡œí”¼ê°€ ê°ì†Œí•˜ë©´ ê·¸ë§Œí¼ ëª¨ë¸ ë‚´ë¶€ì— ì •ë³´ ì´ë“(information Gain)ì„ ì–»ì—ˆë‹¤ê³  ë³´ëŠ” ê´€ì ì´ë‹¤.  ì—”íŠ¸ë¡œí”¼ ì¦ê°€ê°€ ì •ë³´ ì†ì‹¤ëŸ‰ì´ë¼ê³  ì •ì˜í•˜ëŠ” ê²ƒì˜ ë°˜ëŒ€ ê´€ì ì´ë‹¤.

![Untitled](image/28-%2017.png)

e(S) êµ¬í•˜ëŠ” ìˆ˜ì‹

![Untitled](image/28-%2018.png)

Fê°€ Outlookì¼ ë•Œ, ì¦‰ Sunny, Overcast, Rainyì¤‘ í•˜ë‚˜ì¼ ë•Œì˜ ì—”íŠ¸ë¡œí”¼

![Untitled](image/28-%2019.png)

ë”°ë¼ì„œ ì •ë³´ ì´ë“ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](image/28-%2020.png)

### code

```python
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier 
from sklearn.model_selection import train_test_split 
from sklearn import metrics
```

```python
import os
csv_path = os.getenv('HOME')+'/aiffel/information_theory/diabetes.csv'

col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']
# load dataset
df = pd.read_csv(csv_path, header=0, names=col_names)
df.head()
```

```python
# ë°ì´í„°ì…‹ ì¤€ë¹„
feature_cols = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']
X = df[feature_cols] # Features
y = df.label # Target variable

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
```

```python
# Decision Tree  ëª¨ë¸ í•™ìŠµ
# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
'''
Accuracy: 0.6753246753246753
'''
```

ì‹œê°í™”

```python
from sklearn.tree import export_graphviz
from six import StringIO  
from IPython.display import Image  
import pydotplus

dot_data = StringIO()
export_graphviz(clf, 
                out_file=dot_data,  
                filled=True, 
                rounded=True,
                special_characters=True, 
                feature_names=feature_cols, 
                class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('diabetes1.png')
Image(graph.create_png(), retina=True)
```

![Untitled](image/28-%2021.png)

Impurity ì¸¡ì •

```python
# ì •ë³´ ì´ë“ì´ ë˜ì§€ ì•Šê³  ë‚¨ì€ Impurity  ì´ëŸ‰ ì¸¡ì •
ccp_path1 = clf.cost_complexity_pruning_path(X_train,y_train)
ccp_path2 = clf.cost_complexity_pruning_path(X_test,y_test)
print(np.mean(ccp_path1.impurities))
print(np.mean(ccp_path2.impurities))
'''
0.15300447927425634
0.14770796419696539
'''
```

Decision Tree 3 depthê¹Œì§€ë§Œ ë°œì „

```python
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3)

# Train Decision Tree Classifer
clf = clf.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
'''
Accuracy: 0.7705627705627706
'''
```

ë‚¨ì€ ì—”íŠ¸ë¡œí”¼ì˜ ì´ëŸ‰

```python
dot_data = StringIO()
export_graphviz(clf, 
                out_file=dot_data,  
                filled=True, 
                rounded=True,
                special_characters=True,
                feature_names = feature_cols,
                class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('diabetes2.png')
Image(graph.create_png(), retina=True)
```

![Untitled](image/28-%2022.png)

```python
# ì •ë³´ì´ë“ì´ ë˜ì§€ ì•Šê³  ë‚¨ì€ Impurity  ì´ëŸ‰ ì¸¡ì •
ccp_path1 = clf.cost_complexity_pruning_path(X_train,y_train)
ccp_path2 = clf.cost_complexity_pruning_path(X_test,y_test)
print(np.mean(ccp_path1.impurities))
print(np.mean(ccp_path2.impurities))
'''
0.7474881472739515
0.6878691771636323
'''
```